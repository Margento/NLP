{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Margento: NLP Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NATURAL LANGUAGE PROCESSING--NLP--A Brief Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHAT IS NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Natural language processing (NLP) is the interaction between computers and human language. \n",
    "More specifically, natural language processing is the computer understanding, analysis, manipulation, and/or generation of natural language ( according to dictionary.com). In a wider context, NLP is a component of artificial intelligence (AI), a field of compuational linguistics, and can employ various types of approaches, including machine learning (ML) and deep learning.\n",
    "\n",
    "(Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it learn for themselves.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If one zooms in, they will realize that a section (the last part) of the dictionary definition above has recently veered into specific directions of its own, and that it is meanwhile a subject in and of itlsef--NLG--, while NLP focuses mainly on understanding, classifying, and analyzing natural language. But these two main directions have indeed always oevrlapped and most likely always will."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The human-computer interaction that constitutes the scope of NLP enables real-world applications like automatic text summarization, sentiment analysis, topic extraction, named entity recognition, parts-of-speech tagging, relationship extraction, stemming, and more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This great variety and complexity arises from the very boundless 'nature of the natural'. For, what is natural language, and how many ways of and purposes for using it are out there. Virtually, an infinity. How could one trade in infinity--and obtain palpable results? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "But before getting to infinity, we have the... digits we can 'count on'. NLP is quintessentially based on the ancient overlapping between speaking and counting, between speech and numbers. E.g. tell & tale (~Zahl) & toll (Germ.), tally & count & recount (Lat.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "NLP METHODS & ALGORITHMS\n",
    "\n",
    "How do we develop NLP algorithms?\n",
    "\n",
    "To start with, what is a human's algorithm for processing human language?\n",
    "\n",
    "Humans 'automatically' break down sentences into units of meaning. They parse blocks of texts for syntactic and semantic units.\n",
    "\n",
    "So we have to first explicitly show the computer how to do this, in a process called tokenization.\n",
    "\n",
    "After tokenization, we can convert the tokens into a matrix (which is called the bag of words model).\n",
    "\n",
    "Once we have a matrix, we can develop machine learning algorithm to train a model and predict scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "MAIN CHARACTERISTICS AND GOALS OF AN NLP/ML ALGORITHM\n",
    "\n",
    "So what will the algorithm be doing?\n",
    "\n",
    "The algorithm is going to be taking in a lot of numerical values, along with the scores that are associated with these values. These values (or variables or dimensions) are called features.\n",
    "\n",
    "We will extract the numerical values from the text.\n",
    "\n",
    "For example, the number of times the word \"computer\" appears in a piece of text can be defined as a feature.\n",
    "\n",
    "The values (features) are derived from the input text, and aggreggated on a per-text basis into feature vectors.\n",
    "\n",
    "Multiple feature vectors are generally placed together into a feature matrix, where each row represents a piece of text (a document).\n",
    "\n",
    "The algorithm will discover which of these features is relevant, and which are not.\n",
    "\n",
    "The relevance is determined by whether or not the features differentiate a high scoring essay/article/poem from a low scoring one.\n",
    "\n",
    "Therefore, we will want to feed the algorithm features that are specific (they measure as few things as possible) and relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "LET US BRIEFLY EXPLAIN THE BASIC TERMS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZATION\n",
    "\n",
    "Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens. While doing so, soemtimes the task also involves throwing away certain characters, such as punctuation.\n",
    "\n",
    "Tokens are frequently equated with words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to distinguish between 3 concepts here:\n",
    "    - TOKENS\n",
    "    - TYPES\n",
    "    - TERMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A TOKEN is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing. A TYPE is the class of all tokens containing the same character sequence. A TERM is a (perhaps normalized) type that is included in the IR system's dictionary. (Source: Manning et al. 2008. Introduction to Information Retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "LET US CONSIDER AN EXAMPLE:\n",
    "    \n",
    "    \"To die, to sleep—\n",
    "No more—and by a sleep to say we end\"\n",
    "\n",
    "Let's count:\n",
    "\n",
    "How many tokens: \n",
    "14?\n",
    "How many types:\n",
    "11?   \n",
    "How many terms:\n",
    "4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "BAG OF WORDS\n",
    "\n",
    "Also known as the vector space model. In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. \n",
    "\n",
    "We track down the terms, and then count the tokens. \n",
    "Or, in plain EN, the algorithm will count how many times a word appears in a document. Those word counts allow us to compare documents and gauge their similarities for applications like search, document classification, and topic modeling.\n",
    "\n",
    "The idea is to analyse and classify different “bags of words.\" And by matching the different categories, we identify which “bag” a certain block of text (test data) comes from. The model represents text as a multiset of words.\n",
    "\n",
    "In mathematics, a multiset (aka bag or mset) is a modification of the concept of a set that, unlike a set, allows for multiple instances for each of its elements. The positive integer number of instances, given for each element is called the multiplicity of this element in the multiset.\n",
    "\n",
    "Then we construct an n X t document-term matrix, where n is the number of documents, and t is the number of unique terms.\n",
    "Each column represents a unique term, and each cell i,j represents how many of term j are in document i.\n",
    "This is called a simple term frequency bag of words. A more refined apprach to the number of occurrences of terms in a document within a corpus is TFIDF (term frequency--inverse document frequency) of which we will speak in a bit.\n",
    "\n",
    "The NLP features in this model are the terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In Python we have a very efficient machine learning library, scikit-learn, deployable and very effective for NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "SCIKIT-LEARN BoW & VECTORIZER USAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We need to import a number of useful packages in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The documents we want to analyze make up a corpus. We need to have that corpus made available to the machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let us suppose we have a corpus of 4 short documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = [\"I am interested in NLP and would like to learn more about vectors and vectorizers for language processing and NLP in general.\",\n",
    "\n",
    "\"For those who like to experiment with vectors in dealing with various data, it might be useful to work with NLP vectorizers.\",\n",
    "\n",
    "\"A significant computational experiment that would involve language will also have to involve vectors, so you will want to look into vectorizers.\",\n",
    "\n",
    "\"Even if you are not interested in NLP, but you are dealing with data, you can use vectors to organize your data.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "#fit_transform both determines which tokens it will count, and how they correspond to entries in the count vector. \n",
    "#It also gives you the count vectors for the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x50 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 72 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 14)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 40)\t1\n",
      "  (0, 41)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 27)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 36)\t1\n",
      "  (0, 24)\t1\n",
      "  (0, 47)\t1\n",
      "  (0, 3)\t3\n",
      "  (0, 28)\t2\n",
      "  (0, 17)\t2\n",
      "  (0, 18)\t1\n",
      "  (0, 2)\t1\n",
      "  (1, 46)\t1\n",
      "  (1, 38)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 26)\t1\n",
      "  (1, 21)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 39)\t1\n",
      "  (1, 10)\t1\n",
      "  :\t:\n",
      "  (2, 32)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 22)\t1\n",
      "  (2, 40)\t1\n",
      "  (2, 41)\t1\n",
      "  (2, 36)\t2\n",
      "  (2, 47)\t1\n",
      "  (3, 49)\t1\n",
      "  (3, 30)\t1\n",
      "  (3, 37)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 29)\t1\n",
      "  (3, 4)\t2\n",
      "  (3, 16)\t1\n",
      "  (3, 11)\t1\n",
      "  (3, 48)\t3\n",
      "  (3, 9)\t2\n",
      "  (3, 10)\t1\n",
      "  (3, 45)\t1\n",
      "  (3, 41)\t1\n",
      "  (3, 36)\t1\n",
      "  (3, 28)\t1\n",
      "  (3, 17)\t1\n",
      "  (3, 18)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The in-built tokenizer extracts words of at least two characters (but the parameters can be adjusted by the user). \n",
    "We can call that function and use it specifically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['am',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'nlp',\n",
       " 'and',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'more',\n",
       " 'about',\n",
       " 'vectors',\n",
       " 'and',\n",
       " 'vectorizers',\n",
       " 'for',\n",
       " 'language',\n",
       " 'processing']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze(\"I am interested in NLP and would like to learn more about vectors and vectorizers for language processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list0 = list(analyze(\"I am interested in NLP and would like to learn more about vectors and vectorizers for language processing.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Remember in the Shakesperean quote above we reduced the number of terms significantly by counting some of them out? Which ones? \n",
    "The stopwords, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we get the machine to remove stopwords? \n",
    "We need a list of them and we need to tell the machine those are stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the list based off of the nltk corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_list = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to redefine the vectorizer so that its configuration takes into account (and therefore counts out) stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = CountVectorizer(stop_words = stop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we redefine the analyzer based on this new vectorizer that removes the stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now run it again on the same document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['interested',\n",
       " 'nlp',\n",
       " 'would',\n",
       " 'like',\n",
       " 'learn',\n",
       " 'vectors',\n",
       " 'vectorizers',\n",
       " 'language',\n",
       " 'processing']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze(\"I am interested in NLP and would like to learn more about vectors and vectorizers for language processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we need to count them again (on our... digits) as we earlier did with the Shakesperean lines to see how many terms were removed, or can we automatize this? (EXERCISE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = analyze(\"I am interested in NLP and would like to learn more about vectors and vectorizers for language processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(17 - len(list1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorization of the whole corpus is also going to look different now that we have factored in stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x26 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 40 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 25)\t1\n",
      "  (0, 14)\t2\n",
      "  (0, 7)\t1\n",
      "  (1, 24)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 20)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 21)\t1\n",
      "  (1, 22)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 14)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 23)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 8)\t2\n",
      "  (2, 1)\t1\n",
      "  (2, 17)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 21)\t1\n",
      "  (2, 22)\t1\n",
      "  (2, 25)\t1\n",
      "  (3, 15)\t1\n",
      "  (3, 18)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 2)\t2\n",
      "  (3, 3)\t1\n",
      "  (3, 22)\t1\n",
      "  (3, 14)\t1\n",
      "  (3, 7)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it prints out that way, X is actually a matrix. To better understand its content we can convert it to an array:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 1],\n",
       "       [0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        1, 1, 0, 1],\n",
       "       [0, 0, 2, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what do these numbers represent? Can we trace them back to the initial words and thus retrieve the interpretation of the columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['also',\n",
       " 'computational',\n",
       " 'data',\n",
       " 'dealing',\n",
       " 'even',\n",
       " 'experiment',\n",
       " 'general',\n",
       " 'interested',\n",
       " 'involve',\n",
       " 'language',\n",
       " 'learn',\n",
       " 'like',\n",
       " 'look',\n",
       " 'might',\n",
       " 'nlp',\n",
       " 'organize',\n",
       " 'processing',\n",
       " 'significant',\n",
       " 'use',\n",
       " 'useful',\n",
       " 'various',\n",
       " 'vectorizers',\n",
       " 'vectors',\n",
       " 'want',\n",
       " 'work',\n",
       " 'would']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we track automatically which term is which column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2 = vectorizer.get_feature_names() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'involve'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list2[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this term occur in a certain document? Say the second document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = list(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[1][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[2][8] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE \n",
    "\n",
    "Please write the code to find out whether the last document contains this term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What if we want to get a better sense of what happens in terms of terms not only in individual document but also across the corpus/archive/dataset?\n",
    "For now we know very accurately how important a term is for every single document. \n",
    "But how relevant is it for the whole corpus and more importantly, where does a term's frequency place a document in comparison to to the other documents in the corpus/archive/dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let us look into a plain intuitive down-to-earth introduction to the concept:\n",
    "    https://bit.ly/2HZATvi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see now how we can apply this concept in measuring similarities between documents in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "#What does the paramater smooth_idf do? If a word from vocabulary was never seen in the train data, \n",
    "#but occures in the test, smooth_idf allows it to be successfully processed. \n",
    "#See a relevant example here https://bit.ly/2r7yPYn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf = transformer.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x26 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 40 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 25)\t0.2752418429149148\n",
      "  (0, 22)\t0.16256226633758372\n",
      "  (0, 21)\t0.20932851602003819\n",
      "  (0, 16)\t0.3879214194922459\n",
      "  (0, 14)\t0.41865703204007637\n",
      "  (0, 11)\t0.2752418429149148\n",
      "  (0, 10)\t0.3879214194922459\n",
      "  (0, 9)\t0.2752418429149148\n",
      "  (0, 7)\t0.2752418429149148\n",
      "  (0, 6)\t0.3879214194922459\n",
      "  (1, 24)\t0.3842826314790139\n",
      "  (1, 22)\t0.16103739661802227\n",
      "  (1, 21)\t0.20736496861933432\n",
      "  (1, 20)\t0.3842826314790139\n",
      "  (1, 19)\t0.3842826314790139\n",
      "  (1, 14)\t0.20736496861933432\n",
      "  (1, 13)\t0.3842826314790139\n",
      "  (1, 11)\t0.2726600140485181\n",
      "  (1, 5)\t0.2726600140485181\n",
      "  (1, 3)\t0.2726600140485181\n",
      "  (1, 2)\t0.2726600140485181\n",
      "  (2, 25)\t0.21415439885527446\n",
      "  (2, 23)\t0.30182575990136634\n",
      "  (2, 22)\t0.12648303780918257\n",
      "  (2, 21)\t0.16286994025612517\n",
      "  (2, 17)\t0.30182575990136634\n",
      "  (2, 12)\t0.30182575990136634\n",
      "  (2, 9)\t0.21415439885527446\n",
      "  (2, 8)\t0.6036515198027327\n",
      "  (2, 5)\t0.21415439885527446\n",
      "  (2, 1)\t0.30182575990136634\n",
      "  (2, 0)\t0.30182575990136634\n",
      "  (3, 22)\t0.16452841156123668\n",
      "  (3, 18)\t0.39261322075259175\n",
      "  (3, 15)\t0.39261322075259175\n",
      "  (3, 14)\t0.21186028597637277\n",
      "  (3, 7)\t0.2785708161569142\n",
      "  (3, 4)\t0.39261322075259175\n",
      "  (3, 3)\t0.2785708161569142\n",
      "  (3, 2)\t0.5571416323138284\n"
     ]
    }
   ],
   "source": [
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.38792142, 0.27524184, 0.        , 0.27524184,\n",
       "        0.38792142, 0.27524184, 0.        , 0.        , 0.41865703,\n",
       "        0.        , 0.38792142, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.20932852, 0.16256227, 0.        , 0.        ,\n",
       "        0.27524184],\n",
       "       [0.        , 0.        , 0.27266001, 0.27266001, 0.        ,\n",
       "        0.27266001, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.27266001, 0.        , 0.38428263, 0.20736497,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.38428263,\n",
       "        0.38428263, 0.20736497, 0.1610374 , 0.        , 0.38428263,\n",
       "        0.        ],\n",
       "       [0.30182576, 0.30182576, 0.        , 0.        , 0.        ,\n",
       "        0.2141544 , 0.        , 0.        , 0.60365152, 0.2141544 ,\n",
       "        0.        , 0.        , 0.30182576, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.30182576, 0.        , 0.        ,\n",
       "        0.        , 0.16286994, 0.12648304, 0.30182576, 0.        ,\n",
       "        0.2141544 ],\n",
       "       [0.        , 0.        , 0.55714163, 0.27857082, 0.39261322,\n",
       "        0.        , 0.        , 0.27857082, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.21186029,\n",
       "        0.39261322, 0.        , 0.        , 0.39261322, 0.        ,\n",
       "        0.        , 0.        , 0.16452841, 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE \n",
    "\n",
    "How can we find out which terms these values correspond to? For instance, for document 1, which term corresponds to the first occurence of the score 0.27266001?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we get a breakdown of terms and tfidfs for each term in each document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "list3 = list(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.2726600140485181\n",
      "3 0.2726600140485181\n",
      "5 0.2726600140485181\n",
      "11 0.2726600140485181\n",
      "13 0.3842826314790139\n",
      "14 0.20736496861933432\n",
      "19 0.3842826314790139\n",
      "20 0.3842826314790139\n",
      "21 0.20736496861933432\n",
      "22 0.16103739661802227\n",
      "24 0.3842826314790139\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    if list3[1][i] != 0:\n",
    "        print(i, list3[1][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us say we want to do that for document 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "also 0.0\n",
      "computational 0.0\n",
      "data 0.0\n",
      "dealing 0.0\n",
      "even 0.0\n",
      "experiment 0.0\n",
      "general 0.3879214194922459\n",
      "interested 0.2752418429149148\n",
      "involve 0.0\n",
      "language 0.2752418429149148\n",
      "learn 0.3879214194922459\n",
      "like 0.2752418429149148\n",
      "look 0.0\n",
      "might 0.0\n",
      "nlp 0.41865703204007637\n",
      "organize 0.0\n",
      "processing 0.3879214194922459\n",
      "significant 0.0\n",
      "use 0.0\n",
      "useful 0.0\n",
      "various 0.0\n",
      "vectorizers 0.20932851602003819\n",
      "vectors 0.16256226633758372\n",
      "want 0.0\n",
      "work 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    print(list2[i], list3[0][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we sort the the items above in increasing/decreasing order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "list4 = []\n",
    "for i in range(25):\n",
    "    list4.append((list2[i], list3[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nlp', 0.41865703204007637),\n",
       " ('general', 0.3879214194922459),\n",
       " ('learn', 0.3879214194922459),\n",
       " ('processing', 0.3879214194922459),\n",
       " ('interested', 0.2752418429149148),\n",
       " ('language', 0.2752418429149148),\n",
       " ('like', 0.2752418429149148),\n",
       " ('vectorizers', 0.20932851602003819),\n",
       " ('vectors', 0.16256226633758372),\n",
       " ('also', 0.0),\n",
       " ('computational', 0.0),\n",
       " ('data', 0.0),\n",
       " ('dealing', 0.0),\n",
       " ('even', 0.0),\n",
       " ('experiment', 0.0),\n",
       " ('involve', 0.0),\n",
       " ('look', 0.0),\n",
       " ('might', 0.0),\n",
       " ('organize', 0.0),\n",
       " ('significant', 0.0),\n",
       " ('use', 0.0),\n",
       " ('useful', 0.0),\n",
       " ('various', 0.0),\n",
       " ('want', 0.0),\n",
       " ('work', 0.0)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list4, key = lambda t:t[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = [('correlation', float)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = np.matrix((tfidf * tfidf.T).A, dtype=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = np.matrix((tfidf * tfidf.T).A, dtype=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(1.        ,) (0.23144825,) (0.17254319,) (0.19211725,)]\n",
      " [(0.23144825,) (1.        ,) (0.11253336,) (0.298293  ,)]\n",
      " [(0.17254319,) (0.11253336,) (1.        ,) (0.02081005,)]\n",
      " [(0.19211725,) (0.298293  ,) (0.02081005,) (1.        ,)]]\n"
     ]
    }
   ],
   "source": [
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "How do we sort this list in increasing/decreasing order?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let us take another example with, this time, larger documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus1 = [\"Mr. Utterson the lawyer was a man of a rugged countenance that was never lighted by a smile; cold, scanty and embarrassed in discourse; backward in sentiment; lean, long, dusty, dreary and yet somehow lovable. At friendly meetings, and when the wine was to his taste, something eminently human beaconed from his eye; something indeed which never found its way into his talk, but which spoke not only in these silent symbols of the after-dinner face, but more often and loudly in the acts of his life. He was austere with himself; drank gin when he was alone, to mortify a taste for vintages; and though he enjoyed the theatre, had not crossed the doors of one for twenty years. But he had an approved tolerance for others; sometimes wondering, almost with envy, at the high pressure of spirits involved in their misdeeds; and in any extremity inclined to help rather than to reprove. I incline to Cain’s heresy, he used to say quaintly: I let my brother go to the devil in his own way. In this character, it was frequently his fortune to be the last reputable acquaintance and the last good influence in the lives of downgoing men. And to such as these, so long as they came about his chambers, he never marked a shade of change in his demeanour. No doubt the feat was easy to Mr. Utterson; for he was undemonstrative at the best, and even his friendship seemed to be founded in a similar catholicity of good-nature. It is the mark of a modest man to accept his friendly circle ready-made from the hands of opportunity; and that was the lawyer’s way. His friends were those of his own blood or those whom he had known the longest; his affections, like ivy, were the growth of time, they implied no aptness in the object. Hence, no doubt the bond that united him to Mr. Richard Enfield, his distant kinsman, the well-known man about town. It was a nut to crack for many, what these two could see in each other, or what subject they could find in common. It was reported by those who encountered them in their Sunday walks, that they said nothing, looked singularly dull and would hail with obvious relief the appearance of a friend. For all that, the two men put the greatest store by these excursions, counted them the chief jewel of each week, and not only set aside occasions of pleasure, but even resisted the calls of business, that they might enjoy them uninterrupted. It chanced on one of these rambles that their way led them down a by-street in a busy quarter of London. The street was small and what is called quiet, but it drove a thriving trade on the weekdays. The inhabitants were all doing well, it seemed and all emulously hoping to do better still, and laying out the surplus of their grains in coquetry; so that the shop fronts stood along that thoroughfare with an air of invitation, like rows of smiling saleswomen. Even on Sunday, when it veiled its more florid charms and lay comparatively empty of passage, the street shone out in contrast to its dingy neighbourhood, like a fire in a forest; and with its freshly painted shutters, well-polished brasses, and general cleanliness and gaiety of note, instantly caught and pleased the eye of the passenger.\", \"One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off any moment.  His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he looked. What has happened to me? he thought.  It was not a dream.  His room, a proper human room although a little too small, lay peacefully between its four familiar walls.  A collection of textile samples lay spread out on the table - Samsa was a travelling salesman - and above it there hung a picture that he had recently cut out of an illustrated magazine and housed in a nice, gilded frame.  It showed a lady fitted out with a fur hat and fur boa who sat upright, raising a heavy fur muff that covered the whole of her lower arm towards the viewer. Gregor then turned to look out the window at the dull weather. Drops of rain could be heard hitting the pane, which made him feel quite sad.  How about if I sleep a little bit longer and forget all this nonsense, he thought, but that was something he was unable to do because he was used to sleeping on his right, and in his present state could not get into that position.  However hard he threw himself onto his right, he always rolled back to where he was.  He must have tried it a hundred times, shut his eyes so that he would not have to look at the floundering legs, and only stopped when he began to feel a mild, dull pain there that he had never felt before.\", \"It is impossible to follow here the varying fortunes of the Italian states, which in 1507 were controlled by France, Spain, and Germany, with results that have lasted to our day; we are concerned with those events, and with the three great actors in them, so far only as they impinge on the personality of Machiavelli. He had several meetings with Louis XII of France, and his estimate of that monarch's character has already been alluded to. Machiavelli has painted Ferdinand of Aragon as the man who accomplished great things under the cloak of religion, but who in reality had no mercy, faith, humanity, or integrity; and who, had he allowed himself to be influenced by such motives, would have been ruined. The Emperor Maximilian was one of the most interesting men of the age, and his character has been drawn by many hands; but Machiavelli, who was an envoy at his court in 1507-8, reveals the secret of his many failures when he describes him as a secretive man, without force of character--ignoring the human agencies necessary to carry his schemes into effect, and never insisting on the fulfilment of his wishes. The remaining years of Machiavelli's official career were filled with events arising out of the League of Cambrai, made in 1508 between the three great European powers already mentioned and the pope, with the object of crushing the Venetian Republic. This result was attained in the battle of Vaila, when Venice lost in one day all that she had won in eight hundred years. Florence had a difficult part to play during these events, complicated as they were by the feud which broke out between the pope and the French, because friendship with France had dictated the entire policy of the Republic. When, in 1511, Julius II finally formed the Holy League against France, and with the assistance of the Swiss drove the French out of Italy, Florence lay at the mercy of the Pope, and had to submit to his terms, one of which was that the Medici should be restored. The return of the Medici to Florence on 1st September 1512, and the consequent fall of the Republic, was the signal for the dismissal of Machiavelli and his friends, and thus put an end to his public career, for, as we have seen, he died without regaining office.\", \"Having had some time at my disposal when in London, I had visited the British Museum, and made search among the books and maps in the library regarding Transylvania; it had struck me that some foreknowledge of the country could hardly fail to have some importance in dealing with a nobleman of that country. I find that the district he named is in the extreme east of the country, just on the borders of three states, Transylvania, Moldavia and Bukovina, in the midst of the Carpathian mountains; one of the wildest and least known portions of Europe. I was not able to light on any map or work giving the exact locality of the Castle Dracula, as there are no maps of this country as yet to compare with our own Ordnance Survey maps; but I found that Bistritz, the post town named by Count Dracula, is a fairly well-known place. I shall enter here some of my notes, as they may refresh my memory when I talk over my travels with Mina. In the population of Transylvania there are four distinct nationalities: Saxons in the South, and mixed with them the Wallachs, who are the descendants of the Dacians; Magyars in the West, and Szekelys in the East and North. I am going among the latter, who claim to be descended from Attila and the Huns. This may be so, for when the Magyars conquered the country in the eleventh century they found the Huns settled in it. I read that every known superstition in the world is gathered into the horseshoe of the Carpathians, as if it were the centre of some sort of imaginative whirlpool; if so my stay may be very interesting. (Mem., I must ask the Count all about them.) I did not sleep well, though my bed was comfortable enough, for I had all sorts of queer dreams. There was a dog howling all night under my window, which may have had something to do with it; or it may have been the paprika, for I had to drink up all the water in my carafe, and was still thirsty. Towards morning I slept and was wakened by the continuous knocking at my door, so I guess I must have been sleeping soundly then. I had for breakfast more paprika, and a sort of porridge of maize flour which they said was mamaliga, and egg-plant stuffed with forcemeat, a very excellent dish, which they call impletata. (Mem., get recipe for this also.) I had to hurry breakfast, for the train started a little before eight, or rather it ought to have done so, for after rushing to the station at 7:30 I had to sit in the carriage for more than an hour before we began to move. It seems to me that the further east you go the more unpunctual are the trains. What ought they to be in China?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first pages of Dr Jekyll and Mr Hyde, Metamorphosis, The Prince, and Dracula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = vectorizer.fit_transform(corpus1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x639 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 713 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 421)\t1\n",
      "  (0, 429)\t1\n",
      "  (0, 80)\t1\n",
      "  (0, 302)\t1\n",
      "  (0, 399)\t1\n",
      "  (0, 240)\t1\n",
      "  (0, 92)\t1\n",
      "  (0, 242)\t1\n",
      "  (0, 57)\t1\n",
      "  (0, 432)\t1\n",
      "  (0, 514)\t1\n",
      "  (0, 416)\t1\n",
      "  (0, 232)\t1\n",
      "  (0, 220)\t1\n",
      "  (0, 210)\t1\n",
      "  (0, 392)\t1\n",
      "  (0, 132)\t1\n",
      "  (0, 106)\t1\n",
      "  (0, 510)\t1\n",
      "  (0, 420)\t1\n",
      "  (0, 169)\t1\n",
      "  (0, 98)\t1\n",
      "  (0, 320)\t1\n",
      "  (0, 87)\t1\n",
      "  (0, 213)\t1\n",
      "  :\t:\n",
      "  (3, 227)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 261)\t1\n",
      "  (3, 334)\t1\n",
      "  (3, 44)\t1\n",
      "  (3, 153)\t1\n",
      "  (3, 379)\t1\n",
      "  (3, 546)\t1\n",
      "  (3, 337)\t1\n",
      "  (3, 485)\t1\n",
      "  (3, 209)\t1\n",
      "  (3, 109)\t1\n",
      "  (3, 582)\t1\n",
      "  (3, 622)\t2\n",
      "  (3, 578)\t1\n",
      "  (3, 314)\t3\n",
      "  (3, 349)\t1\n",
      "  (3, 248)\t1\n",
      "  (3, 453)\t1\n",
      "  (3, 409)\t1\n",
      "  (3, 572)\t1\n",
      "  (3, 563)\t1\n",
      "  (3, 225)\t2\n",
      "  (3, 530)\t1\n",
      "  (3, 638)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1 = X1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 1 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [2 1 1 ... 1 2 0]\n",
      " [0 0 0 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_words = vectorizer.get_feature_names() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1507'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'books'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_words[55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "EXERCISE\n",
    "In which of the documents does it occur? And how many times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Your code here \n",
    "for i in range(4):\n",
    "    print(Y1[i][55])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1 = transformer.fit_transform(Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x639 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 713 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.04007232,\n",
       "        0.04007232],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.12593762, 0.06296881, 0.06296881, ..., 0.06296881, 0.0893565 ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.04296216]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1_list = list(tfidf1.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_document3 = []\n",
    "for i in range(638):\n",
    "    list_document3.append((list_words[i], tfidf1_list[3][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1507', 0.0), ('1508', 0.0), ('1511', 0.0), ('1512', 0.0), ('1st', 0.0), ('30', 0.0605501730294036), ('able', 0.04296215773608023), ('accept', 0.0), ('accomplished', 0.0), ('acquaintance', 0.0), ('actors', 0.0), ('acts', 0.0), ('affections', 0.0), ('age', 0.0), ('agencies', 0.0), ('air', 0.0), ('allowed', 0.0), ('alluded', 0.0), ('almost', 0.0), ('alone', 0.0), ('along', 0.0), ('already', 0.0), ('also', 0.0605501730294036), ('although', 0.0), ('always', 0.0), ('among', 0.1211003460588072), ('appearance', 0.0), ('approved', 0.0), ('aptness', 0.0), ('aragon', 0.0), ('arches', 0.0), ('arising', 0.0), ('arm', 0.0), ('armour', 0.0), ('aside', 0.0), ('ask', 0.0605501730294036), ('assistance', 0.0), ('attained', 0.0), ('attila', 0.0605501730294036), ('austere', 0.0), ('back', 0.0), ('backward', 0.0), ('battle', 0.0), ('beaconed', 0.0), ('bed', 0.04296215773608023), ('bedding', 0.0), ('began', 0.04296215773608023), ('belly', 0.0), ('best', 0.0), ('better', 0.0), ('bistritz', 0.0605501730294036), ('bit', 0.0), ('blood', 0.0), ('boa', 0.0), ('bond', 0.0), ('books', 0.0605501730294036), ('borders', 0.0605501730294036), ('brasses', 0.0), ('breakfast', 0.1211003460588072), ('british', 0.0605501730294036), ('broke', 0.0), ('brother', 0.0), ('brown', 0.0), ('bukovina', 0.0605501730294036), ('business', 0.0), ('busy', 0.0), ('cain', 0.0), ('call', 0.0605501730294036), ('called', 0.0), ('calls', 0.0), ('cambrai', 0.0), ('came', 0.0), ('carafe', 0.0605501730294036), ('career', 0.0), ('carpathian', 0.0605501730294036), ('carpathians', 0.0605501730294036), ('carriage', 0.0605501730294036), ('carry', 0.0), ('castle', 0.0605501730294036), ('catholicity', 0.0), ('caught', 0.0), ('centre', 0.0605501730294036), ('century', 0.0605501730294036), ('chambers', 0.0), ('chanced', 0.0), ('change', 0.0), ('character', 0.0), ('charms', 0.0), ('chief', 0.0), ('china', 0.0605501730294036), ('circle', 0.0), ('claim', 0.0605501730294036), ('cleanliness', 0.0), ('cloak', 0.0), ('cold', 0.0), ('collection', 0.0), ('comfortable', 0.0605501730294036), ('common', 0.0), ('comparatively', 0.0), ('compare', 0.0605501730294036), ('compared', 0.0), ('complicated', 0.0), ('concerned', 0.0), ('conquered', 0.0605501730294036), ('consequent', 0.0), ('continuous', 0.0605501730294036), ('contrast', 0.0), ('controlled', 0.0), ('coquetry', 0.0), ('could', 0.03267382832737585), ('count', 0.1211003460588072), ('counted', 0.0), ('countenance', 0.0), ('country', 0.302750865147018), ('court', 0.0), ('cover', 0.0), ('covered', 0.0), ('crack', 0.0), ('crossed', 0.0), ('crushing', 0.0), ('cut', 0.0), ('dacians', 0.0605501730294036), ('day', 0.0), ('dealing', 0.0605501730294036), ('demeanour', 0.0), ('descendants', 0.0605501730294036), ('descended', 0.0605501730294036), ('describes', 0.0), ('devil', 0.0), ('dictated', 0.0), ('died', 0.0), ('difficult', 0.0), ('dingy', 0.0), ('dinner', 0.0), ('discourse', 0.0), ('dish', 0.0605501730294036), ('dismissal', 0.0), ('disposal', 0.0605501730294036), ('distant', 0.0), ('distinct', 0.0605501730294036), ('district', 0.0605501730294036), ('divided', 0.0), ('dog', 0.0605501730294036), ('domed', 0.0), ('done', 0.0605501730294036), ('door', 0.0605501730294036), ('doors', 0.0), ('doubt', 0.0), ('downgoing', 0.0), ('dracula', 0.1211003460588072), ('drank', 0.0), ('drawn', 0.0), ('dream', 0.0), ('dreams', 0.04296215773608023), ('dreary', 0.0), ('drink', 0.0605501730294036), ('drops', 0.0), ('drove', 0.0), ('dull', 0.0), ('dusty', 0.0), ('east', 0.1816505190882108), ('easy', 0.0), ('effect', 0.0), ('egg', 0.0605501730294036), ('eight', 0.04296215773608023), ('eleventh', 0.0605501730294036), ('embarrassed', 0.0), ('eminently', 0.0), ('emperor', 0.0), ('empty', 0.0), ('emulously', 0.0), ('encountered', 0.0), ('end', 0.0), ('enfield', 0.0), ('enjoy', 0.0), ('enjoyed', 0.0), ('enough', 0.0605501730294036), ('enter', 0.0605501730294036), ('entire', 0.0), ('envoy', 0.0), ('envy', 0.0), ('estimate', 0.0), ('europe', 0.0605501730294036), ('european', 0.0), ('even', 0.0), ('events', 0.0), ('every', 0.0605501730294036), ('exact', 0.0605501730294036), ('excellent', 0.0605501730294036), ('excursions', 0.0), ('extreme', 0.0605501730294036), ('extremity', 0.0), ('eye', 0.0), ('eyes', 0.0), ('face', 0.0), ('fail', 0.0605501730294036), ('failures', 0.0), ('fairly', 0.0605501730294036), ('faith', 0.0), ('fall', 0.0), ('familiar', 0.0), ('far', 0.0), ('feat', 0.0), ('feel', 0.0), ('felt', 0.0), ('ferdinand', 0.0), ('feud', 0.0), ('filled', 0.0), ('finally', 0.0), ('find', 0.04296215773608023), ('fire', 0.0), ('fitted', 0.0), ('florence', 0.0), ('florid', 0.0), ('floundering', 0.0), ('flour', 0.0605501730294036), ('follow', 0.0), ('force', 0.0), ('forcemeat', 0.0605501730294036), ('foreknowledge', 0.0605501730294036), ('forest', 0.0), ('forget', 0.0), ('formed', 0.0), ('fortune', 0.0), ('fortunes', 0.0), ('found', 0.0653476566547517), ('founded', 0.0), ('four', 0.04296215773608023), ('frame', 0.0), ('france', 0.0), ('french', 0.0), ('frequently', 0.0), ('freshly', 0.0), ('friend', 0.0), ('friendly', 0.0), ('friends', 0.0), ('friendship', 0.0), ('fronts', 0.0), ('fulfilment', 0.0), ('fur', 0.0), ('gaiety', 0.0), ('gathered', 0.0605501730294036), ('general', 0.0), ('germany', 0.0), ('get', 0.04296215773608023), ('gilded', 0.0), ('gin', 0.0), ('giving', 0.0605501730294036), ('go', 0.04296215773608023), ('going', 0.0605501730294036), ('good', 0.0), ('grains', 0.0), ('great', 0.0), ('greatest', 0.0), ('gregor', 0.0), ('growth', 0.0), ('guess', 0.0605501730294036), ('hail', 0.0), ('hands', 0.0), ('happened', 0.0), ('hard', 0.0), ('hardly', 0.04296215773608023), ('hat', 0.0), ('head', 0.0), ('heard', 0.0), ('heavy', 0.0), ('help', 0.0), ('helplessly', 0.0), ('hence', 0.0), ('heresy', 0.0), ('high', 0.0), ('hitting', 0.0), ('holy', 0.0), ('hoping', 0.0), ('horrible', 0.0), ('horseshoe', 0.0605501730294036), ('hour', 0.0605501730294036), ('housed', 0.0), ('however', 0.0), ('howling', 0.0605501730294036), ('human', 0.0), ('humanity', 0.0), ('hundred', 0.0), ('hung', 0.0), ('huns', 0.1211003460588072), ('hurry', 0.0605501730294036), ('ignoring', 0.0), ('ii', 0.0), ('illustrated', 0.0), ('imaginative', 0.0605501730294036), ('impinge', 0.0), ('impletata', 0.0605501730294036), ('implied', 0.0), ('importance', 0.0605501730294036), ('impossible', 0.0), ('incline', 0.0), ('inclined', 0.0), ('indeed', 0.0), ('influence', 0.0), ('influenced', 0.0), ('inhabitants', 0.0), ('insisting', 0.0), ('instantly', 0.0), ('integrity', 0.0), ('interesting', 0.04296215773608023), ('invitation', 0.0), ('involved', 0.0), ('italian', 0.0), ('italy', 0.0), ('ivy', 0.0), ('jewel', 0.0), ('julius', 0.0), ('kinsman', 0.0), ('knocking', 0.0605501730294036), ('known', 0.1288864732082407), ('lady', 0.0), ('last', 0.0), ('lasted', 0.0), ('latter', 0.0605501730294036), ('lawyer', 0.0), ('lay', 0.0), ('laying', 0.0), ('league', 0.0), ('lean', 0.0), ('least', 0.0605501730294036), ('led', 0.0), ('legs', 0.0), ('let', 0.0), ('library', 0.0605501730294036), ('life', 0.0), ('lifted', 0.0), ('light', 0.0605501730294036), ('lighted', 0.0), ('like', 0.0), ('little', 0.04296215773608023), ('lives', 0.0), ('locality', 0.0605501730294036), ('london', 0.04296215773608023), ('long', 0.0), ('longer', 0.0), ('longest', 0.0), ('look', 0.0), ('looked', 0.0), ('lost', 0.0), ('loudly', 0.0), ('louis', 0.0), ('lovable', 0.0), ('lower', 0.0), ('machiavelli', 0.0), ('made', 0.025374142442756865), ('magazine', 0.0), ('magyars', 0.1211003460588072), ('maize', 0.0605501730294036), ('mamaliga', 0.0605501730294036), ('man', 0.0), ('many', 0.0), ('map', 0.0605501730294036), ('maps', 0.1816505190882108), ('mark', 0.0), ('marked', 0.0), ('maximilian', 0.0), ('may', 0.302750865147018), ('medici', 0.0), ('meetings', 0.0), ('mem', 0.1211003460588072), ('memory', 0.0605501730294036), ('men', 0.0), ('mentioned', 0.0), ('mercy', 0.0), ('midst', 0.0605501730294036), ('might', 0.0), ('mild', 0.0), ('mina', 0.0605501730294036), ('misdeeds', 0.0), ('mixed', 0.0605501730294036), ('modest', 0.0), ('moldavia', 0.0605501730294036), ('moment', 0.0), ('monarch', 0.0), ('morning', 0.04296215773608023), ('mortify', 0.0), ('motives', 0.0), ('mountains', 0.0605501730294036), ('move', 0.0605501730294036), ('mr', 0.0), ('muff', 0.0), ('museum', 0.0605501730294036), ('must', 0.08592431547216046), ('named', 0.1211003460588072), ('nationalities', 0.0605501730294036), ('nature', 0.0), ('necessary', 0.0), ('neighbourhood', 0.0), ('never', 0.0), ('nice', 0.0), ('night', 0.0605501730294036), ('nobleman', 0.0605501730294036), ('nonsense', 0.0), ('north', 0.0605501730294036), ('note', 0.0), ('notes', 0.0605501730294036), ('nothing', 0.0), ('nut', 0.0), ('object', 0.0), ('obvious', 0.0), ('occasions', 0.0), ('office', 0.0), ('official', 0.0), ('often', 0.0), ('one', 0.025374142442756865), ('onto', 0.0), ('opportunity', 0.0), ('ordnance', 0.0605501730294036), ('others', 0.0), ('ought', 0.1211003460588072), ('pain', 0.0), ('painted', 0.0), ('pane', 0.0), ('paprika', 0.1211003460588072), ('part', 0.0), ('passage', 0.0), ('passenger', 0.0), ('peacefully', 0.0), ('personality', 0.0), ('picture', 0.0), ('pitifully', 0.0), ('place', 0.0605501730294036), ('plant', 0.0605501730294036), ('play', 0.0), ('pleased', 0.0), ('pleasure', 0.0), ('policy', 0.0), ('polished', 0.0), ('pope', 0.0), ('population', 0.0605501730294036), ('porridge', 0.0605501730294036), ('portions', 0.0605501730294036), ('position', 0.0), ('post', 0.0605501730294036), ('powers', 0.0), ('present', 0.0), ('pressure', 0.0), ('proper', 0.0), ('public', 0.0), ('put', 0.0), ('quaintly', 0.0), ('quarter', 0.0), ('queer', 0.0605501730294036), ('quiet', 0.0), ('quite', 0.0), ('rain', 0.0), ('raising', 0.0), ('rambles', 0.0), ('rather', 0.04296215773608023), ('read', 0.0605501730294036), ('ready', 0.0), ('reality', 0.0), ('recently', 0.0), ('recipe', 0.0605501730294036), ('refresh', 0.0605501730294036), ('regaining', 0.0), ('regarding', 0.0605501730294036), ('relief', 0.0), ('religion', 0.0), ('remaining', 0.0), ('reported', 0.0), ('reprove', 0.0), ('republic', 0.0), ('reputable', 0.0), ('resisted', 0.0), ('rest', 0.0), ('restored', 0.0), ('result', 0.0), ('results', 0.0), ('return', 0.0), ('reveals', 0.0), ('richard', 0.0), ('right', 0.0), ('rolled', 0.0), ('room', 0.0), ('rows', 0.0), ('rugged', 0.0), ('ruined', 0.0), ('rushing', 0.0605501730294036), ('sad', 0.0), ('said', 0.04296215773608023), ('salesman', 0.0), ('saleswomen', 0.0), ('samples', 0.0), ('samsa', 0.0), ('sat', 0.0), ('saxons', 0.0605501730294036), ('say', 0.0), ('scanty', 0.0), ('schemes', 0.0), ('search', 0.0605501730294036), ('secret', 0.0), ('secretive', 0.0), ('sections', 0.0), ('see', 0.0), ('seemed', 0.0), ('seems', 0.0605501730294036), ('seen', 0.0), ('sentiment', 0.0), ('september', 0.0), ('set', 0.0), ('settled', 0.0605501730294036), ('several', 0.0), ('shade', 0.0), ('shall', 0.0605501730294036), ('shone', 0.0), ('shop', 0.0), ('showed', 0.0), ('shut', 0.0), ('shutters', 0.0), ('signal', 0.0), ('silent', 0.0), ('similar', 0.0), ('singularly', 0.0), ('sit', 0.0605501730294036), ('size', 0.0), ('sleep', 0.04296215773608023), ('sleeping', 0.04296215773608023), ('slept', 0.0605501730294036), ('slide', 0.0), ('slightly', 0.0), ('small', 0.0), ('smile', 0.0), ('smiling', 0.0), ('somehow', 0.0), ('something', 0.03267382832737585), ('sometimes', 0.0), ('sort', 0.1211003460588072), ('sorts', 0.0605501730294036), ('soundly', 0.0605501730294036), ('south', 0.0605501730294036), ('spain', 0.0), ('spirits', 0.0), ('spoke', 0.0), ('spread', 0.0), ('started', 0.0605501730294036), ('state', 0.0), ('states', 0.04296215773608023), ('station', 0.0605501730294036), ('stay', 0.0605501730294036), ('stiff', 0.0), ('still', 0.04296215773608023), ('stood', 0.0), ('stopped', 0.0), ('store', 0.0), ('street', 0.0), ('struck', 0.0605501730294036), ('stuffed', 0.0605501730294036), ('subject', 0.0), ('submit', 0.0), ('sunday', 0.0), ('superstition', 0.0605501730294036), ('surplus', 0.0), ('survey', 0.0605501730294036), ('swiss', 0.0), ('symbols', 0.0), ('szekelys', 0.0605501730294036), ('table', 0.0), ('talk', 0.04296215773608023), ('taste', 0.0), ('terms', 0.0), ('textile', 0.0), ('theatre', 0.0), ('thin', 0.0), ('things', 0.0), ('thirsty', 0.0605501730294036), ('thoroughfare', 0.0), ('though', 0.04296215773608023), ('thought', 0.0), ('three', 0.04296215773608023), ('threw', 0.0), ('thriving', 0.0), ('thus', 0.0), ('time', 0.04296215773608023), ('times', 0.0), ('tolerance', 0.0), ('towards', 0.04296215773608023), ('town', 0.04296215773608023), ('trade', 0.0), ('train', 0.0605501730294036), ('trains', 0.0605501730294036), ('transformed', 0.0), ('transylvania', 0.1816505190882108), ('travelling', 0.0), ('travels', 0.0605501730294036), ('tried', 0.0), ('troubled', 0.0), ('turned', 0.0), ('twenty', 0.0), ('two', 0.0), ('unable', 0.0), ('undemonstrative', 0.0), ('uninterrupted', 0.0), ('united', 0.0), ('unpunctual', 0.0605501730294036), ('upright', 0.0), ('used', 0.0), ('utterson', 0.0), ('vaila', 0.0), ('varying', 0.0), ('veiled', 0.0), ('venetian', 0.0), ('venice', 0.0), ('vermin', 0.0), ('viewer', 0.0), ('vintages', 0.0), ('visited', 0.0605501730294036), ('wakened', 0.0605501730294036), ('walks', 0.0), ('wallachs', 0.0605501730294036), ('walls', 0.0), ('water', 0.0605501730294036), ('waved', 0.0), ('way', 0.0), ('weather', 0.0), ('week', 0.0), ('weekdays', 0.0), ('well', 0.08592431547216046), ('west', 0.0605501730294036), ('whirlpool', 0.0605501730294036), ('whole', 0.0), ('wildest', 0.0605501730294036), ('window', 0.04296215773608023), ('wine', 0.0), ('wishes', 0.0), ('without', 0.0), ('woke', 0.0), ('wondering', 0.0), ('work', 0.0605501730294036), ('world', 0.0605501730294036), ('would', 0.0), ('xii', 0.0), ('years', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "print(list_document3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is a significantly longer list indeed. How do we sort it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1507', 0.0),\n",
       " ('1508', 0.0),\n",
       " ('1511', 0.0),\n",
       " ('1512', 0.0),\n",
       " ('1st', 0.0),\n",
       " ('accept', 0.0),\n",
       " ('accomplished', 0.0),\n",
       " ('acquaintance', 0.0),\n",
       " ('actors', 0.0),\n",
       " ('acts', 0.0),\n",
       " ('affections', 0.0),\n",
       " ('age', 0.0),\n",
       " ('agencies', 0.0),\n",
       " ('air', 0.0),\n",
       " ('allowed', 0.0),\n",
       " ('alluded', 0.0),\n",
       " ('almost', 0.0),\n",
       " ('alone', 0.0),\n",
       " ('along', 0.0),\n",
       " ('already', 0.0),\n",
       " ('although', 0.0),\n",
       " ('always', 0.0),\n",
       " ('appearance', 0.0),\n",
       " ('approved', 0.0),\n",
       " ('aptness', 0.0),\n",
       " ('aragon', 0.0),\n",
       " ('arches', 0.0),\n",
       " ('arising', 0.0),\n",
       " ('arm', 0.0),\n",
       " ('armour', 0.0),\n",
       " ('aside', 0.0),\n",
       " ('assistance', 0.0),\n",
       " ('attained', 0.0),\n",
       " ('austere', 0.0),\n",
       " ('back', 0.0),\n",
       " ('backward', 0.0),\n",
       " ('battle', 0.0),\n",
       " ('beaconed', 0.0),\n",
       " ('bedding', 0.0),\n",
       " ('belly', 0.0),\n",
       " ('best', 0.0),\n",
       " ('better', 0.0),\n",
       " ('bit', 0.0),\n",
       " ('blood', 0.0),\n",
       " ('boa', 0.0),\n",
       " ('bond', 0.0),\n",
       " ('brasses', 0.0),\n",
       " ('broke', 0.0),\n",
       " ('brother', 0.0),\n",
       " ('brown', 0.0),\n",
       " ('business', 0.0),\n",
       " ('busy', 0.0),\n",
       " ('cain', 0.0),\n",
       " ('called', 0.0),\n",
       " ('calls', 0.0),\n",
       " ('cambrai', 0.0),\n",
       " ('came', 0.0),\n",
       " ('career', 0.0),\n",
       " ('carry', 0.0),\n",
       " ('catholicity', 0.0),\n",
       " ('caught', 0.0),\n",
       " ('chambers', 0.0),\n",
       " ('chanced', 0.0),\n",
       " ('change', 0.0),\n",
       " ('character', 0.0),\n",
       " ('charms', 0.0),\n",
       " ('chief', 0.0),\n",
       " ('circle', 0.0),\n",
       " ('cleanliness', 0.0),\n",
       " ('cloak', 0.0),\n",
       " ('cold', 0.0),\n",
       " ('collection', 0.0),\n",
       " ('common', 0.0),\n",
       " ('comparatively', 0.0),\n",
       " ('compared', 0.0),\n",
       " ('complicated', 0.0),\n",
       " ('concerned', 0.0),\n",
       " ('consequent', 0.0),\n",
       " ('contrast', 0.0),\n",
       " ('controlled', 0.0),\n",
       " ('coquetry', 0.0),\n",
       " ('counted', 0.0),\n",
       " ('countenance', 0.0),\n",
       " ('court', 0.0),\n",
       " ('cover', 0.0),\n",
       " ('covered', 0.0),\n",
       " ('crack', 0.0),\n",
       " ('crossed', 0.0),\n",
       " ('crushing', 0.0),\n",
       " ('cut', 0.0),\n",
       " ('day', 0.0),\n",
       " ('demeanour', 0.0),\n",
       " ('describes', 0.0),\n",
       " ('devil', 0.0),\n",
       " ('dictated', 0.0),\n",
       " ('died', 0.0),\n",
       " ('difficult', 0.0),\n",
       " ('dingy', 0.0),\n",
       " ('dinner', 0.0),\n",
       " ('discourse', 0.0),\n",
       " ('dismissal', 0.0),\n",
       " ('distant', 0.0),\n",
       " ('divided', 0.0),\n",
       " ('domed', 0.0),\n",
       " ('doors', 0.0),\n",
       " ('doubt', 0.0),\n",
       " ('downgoing', 0.0),\n",
       " ('drank', 0.0),\n",
       " ('drawn', 0.0),\n",
       " ('dream', 0.0),\n",
       " ('dreary', 0.0),\n",
       " ('drops', 0.0),\n",
       " ('drove', 0.0),\n",
       " ('dull', 0.0),\n",
       " ('dusty', 0.0),\n",
       " ('easy', 0.0),\n",
       " ('effect', 0.0),\n",
       " ('embarrassed', 0.0),\n",
       " ('eminently', 0.0),\n",
       " ('emperor', 0.0),\n",
       " ('empty', 0.0),\n",
       " ('emulously', 0.0),\n",
       " ('encountered', 0.0),\n",
       " ('end', 0.0),\n",
       " ('enfield', 0.0),\n",
       " ('enjoy', 0.0),\n",
       " ('enjoyed', 0.0),\n",
       " ('entire', 0.0),\n",
       " ('envoy', 0.0),\n",
       " ('envy', 0.0),\n",
       " ('estimate', 0.0),\n",
       " ('european', 0.0),\n",
       " ('even', 0.0),\n",
       " ('events', 0.0),\n",
       " ('excursions', 0.0),\n",
       " ('extremity', 0.0),\n",
       " ('eye', 0.0),\n",
       " ('eyes', 0.0),\n",
       " ('face', 0.0),\n",
       " ('failures', 0.0),\n",
       " ('faith', 0.0),\n",
       " ('fall', 0.0),\n",
       " ('familiar', 0.0),\n",
       " ('far', 0.0),\n",
       " ('feat', 0.0),\n",
       " ('feel', 0.0),\n",
       " ('felt', 0.0),\n",
       " ('ferdinand', 0.0),\n",
       " ('feud', 0.0),\n",
       " ('filled', 0.0),\n",
       " ('finally', 0.0),\n",
       " ('fire', 0.0),\n",
       " ('fitted', 0.0),\n",
       " ('florence', 0.0),\n",
       " ('florid', 0.0),\n",
       " ('floundering', 0.0),\n",
       " ('follow', 0.0),\n",
       " ('force', 0.0),\n",
       " ('forest', 0.0),\n",
       " ('forget', 0.0),\n",
       " ('formed', 0.0),\n",
       " ('fortune', 0.0),\n",
       " ('fortunes', 0.0),\n",
       " ('founded', 0.0),\n",
       " ('frame', 0.0),\n",
       " ('france', 0.0),\n",
       " ('french', 0.0),\n",
       " ('frequently', 0.0),\n",
       " ('freshly', 0.0),\n",
       " ('friend', 0.0),\n",
       " ('friendly', 0.0),\n",
       " ('friends', 0.0),\n",
       " ('friendship', 0.0),\n",
       " ('fronts', 0.0),\n",
       " ('fulfilment', 0.0),\n",
       " ('fur', 0.0),\n",
       " ('gaiety', 0.0),\n",
       " ('general', 0.0),\n",
       " ('germany', 0.0),\n",
       " ('gilded', 0.0),\n",
       " ('gin', 0.0),\n",
       " ('good', 0.0),\n",
       " ('grains', 0.0),\n",
       " ('great', 0.0),\n",
       " ('greatest', 0.0),\n",
       " ('gregor', 0.0),\n",
       " ('growth', 0.0),\n",
       " ('hail', 0.0),\n",
       " ('hands', 0.0),\n",
       " ('happened', 0.0),\n",
       " ('hard', 0.0),\n",
       " ('hat', 0.0),\n",
       " ('head', 0.0),\n",
       " ('heard', 0.0),\n",
       " ('heavy', 0.0),\n",
       " ('help', 0.0),\n",
       " ('helplessly', 0.0),\n",
       " ('hence', 0.0),\n",
       " ('heresy', 0.0),\n",
       " ('high', 0.0),\n",
       " ('hitting', 0.0),\n",
       " ('holy', 0.0),\n",
       " ('hoping', 0.0),\n",
       " ('horrible', 0.0),\n",
       " ('housed', 0.0),\n",
       " ('however', 0.0),\n",
       " ('human', 0.0),\n",
       " ('humanity', 0.0),\n",
       " ('hundred', 0.0),\n",
       " ('hung', 0.0),\n",
       " ('ignoring', 0.0),\n",
       " ('ii', 0.0),\n",
       " ('illustrated', 0.0),\n",
       " ('impinge', 0.0),\n",
       " ('implied', 0.0),\n",
       " ('impossible', 0.0),\n",
       " ('incline', 0.0),\n",
       " ('inclined', 0.0),\n",
       " ('indeed', 0.0),\n",
       " ('influence', 0.0),\n",
       " ('influenced', 0.0),\n",
       " ('inhabitants', 0.0),\n",
       " ('insisting', 0.0),\n",
       " ('instantly', 0.0),\n",
       " ('integrity', 0.0),\n",
       " ('invitation', 0.0),\n",
       " ('involved', 0.0),\n",
       " ('italian', 0.0),\n",
       " ('italy', 0.0),\n",
       " ('ivy', 0.0),\n",
       " ('jewel', 0.0),\n",
       " ('julius', 0.0),\n",
       " ('kinsman', 0.0),\n",
       " ('lady', 0.0),\n",
       " ('last', 0.0),\n",
       " ('lasted', 0.0),\n",
       " ('lawyer', 0.0),\n",
       " ('lay', 0.0),\n",
       " ('laying', 0.0),\n",
       " ('league', 0.0),\n",
       " ('lean', 0.0),\n",
       " ('led', 0.0),\n",
       " ('legs', 0.0),\n",
       " ('let', 0.0),\n",
       " ('life', 0.0),\n",
       " ('lifted', 0.0),\n",
       " ('lighted', 0.0),\n",
       " ('like', 0.0),\n",
       " ('lives', 0.0),\n",
       " ('long', 0.0),\n",
       " ('longer', 0.0),\n",
       " ('longest', 0.0),\n",
       " ('look', 0.0),\n",
       " ('looked', 0.0),\n",
       " ('lost', 0.0),\n",
       " ('loudly', 0.0),\n",
       " ('louis', 0.0),\n",
       " ('lovable', 0.0),\n",
       " ('lower', 0.0),\n",
       " ('machiavelli', 0.0),\n",
       " ('magazine', 0.0),\n",
       " ('man', 0.0),\n",
       " ('many', 0.0),\n",
       " ('mark', 0.0),\n",
       " ('marked', 0.0),\n",
       " ('maximilian', 0.0),\n",
       " ('medici', 0.0),\n",
       " ('meetings', 0.0),\n",
       " ('men', 0.0),\n",
       " ('mentioned', 0.0),\n",
       " ('mercy', 0.0),\n",
       " ('might', 0.0),\n",
       " ('mild', 0.0),\n",
       " ('misdeeds', 0.0),\n",
       " ('modest', 0.0),\n",
       " ('moment', 0.0),\n",
       " ('monarch', 0.0),\n",
       " ('mortify', 0.0),\n",
       " ('motives', 0.0),\n",
       " ('mr', 0.0),\n",
       " ('muff', 0.0),\n",
       " ('nature', 0.0),\n",
       " ('necessary', 0.0),\n",
       " ('neighbourhood', 0.0),\n",
       " ('never', 0.0),\n",
       " ('nice', 0.0),\n",
       " ('nonsense', 0.0),\n",
       " ('note', 0.0),\n",
       " ('nothing', 0.0),\n",
       " ('nut', 0.0),\n",
       " ('object', 0.0),\n",
       " ('obvious', 0.0),\n",
       " ('occasions', 0.0),\n",
       " ('office', 0.0),\n",
       " ('official', 0.0),\n",
       " ('often', 0.0),\n",
       " ('onto', 0.0),\n",
       " ('opportunity', 0.0),\n",
       " ('others', 0.0),\n",
       " ('pain', 0.0),\n",
       " ('painted', 0.0),\n",
       " ('pane', 0.0),\n",
       " ('part', 0.0),\n",
       " ('passage', 0.0),\n",
       " ('passenger', 0.0),\n",
       " ('peacefully', 0.0),\n",
       " ('personality', 0.0),\n",
       " ('picture', 0.0),\n",
       " ('pitifully', 0.0),\n",
       " ('play', 0.0),\n",
       " ('pleased', 0.0),\n",
       " ('pleasure', 0.0),\n",
       " ('policy', 0.0),\n",
       " ('polished', 0.0),\n",
       " ('pope', 0.0),\n",
       " ('position', 0.0),\n",
       " ('powers', 0.0),\n",
       " ('present', 0.0),\n",
       " ('pressure', 0.0),\n",
       " ('proper', 0.0),\n",
       " ('public', 0.0),\n",
       " ('put', 0.0),\n",
       " ('quaintly', 0.0),\n",
       " ('quarter', 0.0),\n",
       " ('quiet', 0.0),\n",
       " ('quite', 0.0),\n",
       " ('rain', 0.0),\n",
       " ('raising', 0.0),\n",
       " ('rambles', 0.0),\n",
       " ('ready', 0.0),\n",
       " ('reality', 0.0),\n",
       " ('recently', 0.0),\n",
       " ('regaining', 0.0),\n",
       " ('relief', 0.0),\n",
       " ('religion', 0.0),\n",
       " ('remaining', 0.0),\n",
       " ('reported', 0.0),\n",
       " ('reprove', 0.0),\n",
       " ('republic', 0.0),\n",
       " ('reputable', 0.0),\n",
       " ('resisted', 0.0),\n",
       " ('rest', 0.0),\n",
       " ('restored', 0.0),\n",
       " ('result', 0.0),\n",
       " ('results', 0.0),\n",
       " ('return', 0.0),\n",
       " ('reveals', 0.0),\n",
       " ('richard', 0.0),\n",
       " ('right', 0.0),\n",
       " ('rolled', 0.0),\n",
       " ('room', 0.0),\n",
       " ('rows', 0.0),\n",
       " ('rugged', 0.0),\n",
       " ('ruined', 0.0),\n",
       " ('sad', 0.0),\n",
       " ('salesman', 0.0),\n",
       " ('saleswomen', 0.0),\n",
       " ('samples', 0.0),\n",
       " ('samsa', 0.0),\n",
       " ('sat', 0.0),\n",
       " ('say', 0.0),\n",
       " ('scanty', 0.0),\n",
       " ('schemes', 0.0),\n",
       " ('secret', 0.0),\n",
       " ('secretive', 0.0),\n",
       " ('sections', 0.0),\n",
       " ('see', 0.0),\n",
       " ('seemed', 0.0),\n",
       " ('seen', 0.0),\n",
       " ('sentiment', 0.0),\n",
       " ('september', 0.0),\n",
       " ('set', 0.0),\n",
       " ('several', 0.0),\n",
       " ('shade', 0.0),\n",
       " ('shone', 0.0),\n",
       " ('shop', 0.0),\n",
       " ('showed', 0.0),\n",
       " ('shut', 0.0),\n",
       " ('shutters', 0.0),\n",
       " ('signal', 0.0),\n",
       " ('silent', 0.0),\n",
       " ('similar', 0.0),\n",
       " ('singularly', 0.0),\n",
       " ('size', 0.0),\n",
       " ('slide', 0.0),\n",
       " ('slightly', 0.0),\n",
       " ('small', 0.0),\n",
       " ('smile', 0.0),\n",
       " ('smiling', 0.0),\n",
       " ('somehow', 0.0),\n",
       " ('sometimes', 0.0),\n",
       " ('spain', 0.0),\n",
       " ('spirits', 0.0),\n",
       " ('spoke', 0.0),\n",
       " ('spread', 0.0),\n",
       " ('state', 0.0),\n",
       " ('stiff', 0.0),\n",
       " ('stood', 0.0),\n",
       " ('stopped', 0.0),\n",
       " ('store', 0.0),\n",
       " ('street', 0.0),\n",
       " ('subject', 0.0),\n",
       " ('submit', 0.0),\n",
       " ('sunday', 0.0),\n",
       " ('surplus', 0.0),\n",
       " ('swiss', 0.0),\n",
       " ('symbols', 0.0),\n",
       " ('table', 0.0),\n",
       " ('taste', 0.0),\n",
       " ('terms', 0.0),\n",
       " ('textile', 0.0),\n",
       " ('theatre', 0.0),\n",
       " ('thin', 0.0),\n",
       " ('things', 0.0),\n",
       " ('thoroughfare', 0.0),\n",
       " ('thought', 0.0),\n",
       " ('threw', 0.0),\n",
       " ('thriving', 0.0),\n",
       " ('thus', 0.0),\n",
       " ('times', 0.0),\n",
       " ('tolerance', 0.0),\n",
       " ('trade', 0.0),\n",
       " ('transformed', 0.0),\n",
       " ('travelling', 0.0),\n",
       " ('tried', 0.0),\n",
       " ('troubled', 0.0),\n",
       " ('turned', 0.0),\n",
       " ('twenty', 0.0),\n",
       " ('two', 0.0),\n",
       " ('unable', 0.0),\n",
       " ('undemonstrative', 0.0),\n",
       " ('uninterrupted', 0.0),\n",
       " ('united', 0.0),\n",
       " ('upright', 0.0),\n",
       " ('used', 0.0),\n",
       " ('utterson', 0.0),\n",
       " ('vaila', 0.0),\n",
       " ('varying', 0.0),\n",
       " ('veiled', 0.0),\n",
       " ('venetian', 0.0),\n",
       " ('venice', 0.0),\n",
       " ('vermin', 0.0),\n",
       " ('viewer', 0.0),\n",
       " ('vintages', 0.0),\n",
       " ('walks', 0.0),\n",
       " ('walls', 0.0),\n",
       " ('waved', 0.0),\n",
       " ('way', 0.0),\n",
       " ('weather', 0.0),\n",
       " ('week', 0.0),\n",
       " ('weekdays', 0.0),\n",
       " ('whole', 0.0),\n",
       " ('wine', 0.0),\n",
       " ('wishes', 0.0),\n",
       " ('without', 0.0),\n",
       " ('woke', 0.0),\n",
       " ('wondering', 0.0),\n",
       " ('would', 0.0),\n",
       " ('xii', 0.0),\n",
       " ('years', 0.0),\n",
       " ('made', 0.025374142442756865),\n",
       " ('one', 0.025374142442756865),\n",
       " ('could', 0.03267382832737585),\n",
       " ('something', 0.03267382832737585),\n",
       " ('able', 0.04296215773608023),\n",
       " ('bed', 0.04296215773608023),\n",
       " ('began', 0.04296215773608023),\n",
       " ('dreams', 0.04296215773608023),\n",
       " ('eight', 0.04296215773608023),\n",
       " ('find', 0.04296215773608023),\n",
       " ('four', 0.04296215773608023),\n",
       " ('get', 0.04296215773608023),\n",
       " ('go', 0.04296215773608023),\n",
       " ('hardly', 0.04296215773608023),\n",
       " ('interesting', 0.04296215773608023),\n",
       " ('little', 0.04296215773608023),\n",
       " ('london', 0.04296215773608023),\n",
       " ('morning', 0.04296215773608023),\n",
       " ('rather', 0.04296215773608023),\n",
       " ('said', 0.04296215773608023),\n",
       " ('sleep', 0.04296215773608023),\n",
       " ('sleeping', 0.04296215773608023),\n",
       " ('states', 0.04296215773608023),\n",
       " ('still', 0.04296215773608023),\n",
       " ('talk', 0.04296215773608023),\n",
       " ('though', 0.04296215773608023),\n",
       " ('three', 0.04296215773608023),\n",
       " ('time', 0.04296215773608023),\n",
       " ('towards', 0.04296215773608023),\n",
       " ('town', 0.04296215773608023),\n",
       " ('window', 0.04296215773608023),\n",
       " ('30', 0.0605501730294036),\n",
       " ('also', 0.0605501730294036),\n",
       " ('ask', 0.0605501730294036),\n",
       " ('attila', 0.0605501730294036),\n",
       " ('bistritz', 0.0605501730294036),\n",
       " ('books', 0.0605501730294036),\n",
       " ('borders', 0.0605501730294036),\n",
       " ('british', 0.0605501730294036),\n",
       " ('bukovina', 0.0605501730294036),\n",
       " ('call', 0.0605501730294036),\n",
       " ('carafe', 0.0605501730294036),\n",
       " ('carpathian', 0.0605501730294036),\n",
       " ('carpathians', 0.0605501730294036),\n",
       " ('carriage', 0.0605501730294036),\n",
       " ('castle', 0.0605501730294036),\n",
       " ('centre', 0.0605501730294036),\n",
       " ('century', 0.0605501730294036),\n",
       " ('china', 0.0605501730294036),\n",
       " ('claim', 0.0605501730294036),\n",
       " ('comfortable', 0.0605501730294036),\n",
       " ('compare', 0.0605501730294036),\n",
       " ('conquered', 0.0605501730294036),\n",
       " ('continuous', 0.0605501730294036),\n",
       " ('dacians', 0.0605501730294036),\n",
       " ('dealing', 0.0605501730294036),\n",
       " ('descendants', 0.0605501730294036),\n",
       " ('descended', 0.0605501730294036),\n",
       " ('dish', 0.0605501730294036),\n",
       " ('disposal', 0.0605501730294036),\n",
       " ('distinct', 0.0605501730294036),\n",
       " ('district', 0.0605501730294036),\n",
       " ('dog', 0.0605501730294036),\n",
       " ('done', 0.0605501730294036),\n",
       " ('door', 0.0605501730294036),\n",
       " ('drink', 0.0605501730294036),\n",
       " ('egg', 0.0605501730294036),\n",
       " ('eleventh', 0.0605501730294036),\n",
       " ('enough', 0.0605501730294036),\n",
       " ('enter', 0.0605501730294036),\n",
       " ('europe', 0.0605501730294036),\n",
       " ('every', 0.0605501730294036),\n",
       " ('exact', 0.0605501730294036),\n",
       " ('excellent', 0.0605501730294036),\n",
       " ('extreme', 0.0605501730294036),\n",
       " ('fail', 0.0605501730294036),\n",
       " ('fairly', 0.0605501730294036),\n",
       " ('flour', 0.0605501730294036),\n",
       " ('forcemeat', 0.0605501730294036),\n",
       " ('foreknowledge', 0.0605501730294036),\n",
       " ('gathered', 0.0605501730294036),\n",
       " ('giving', 0.0605501730294036),\n",
       " ('going', 0.0605501730294036),\n",
       " ('guess', 0.0605501730294036),\n",
       " ('horseshoe', 0.0605501730294036),\n",
       " ('hour', 0.0605501730294036),\n",
       " ('howling', 0.0605501730294036),\n",
       " ('hurry', 0.0605501730294036),\n",
       " ('imaginative', 0.0605501730294036),\n",
       " ('impletata', 0.0605501730294036),\n",
       " ('importance', 0.0605501730294036),\n",
       " ('knocking', 0.0605501730294036),\n",
       " ('latter', 0.0605501730294036),\n",
       " ('least', 0.0605501730294036),\n",
       " ('library', 0.0605501730294036),\n",
       " ('light', 0.0605501730294036),\n",
       " ('locality', 0.0605501730294036),\n",
       " ('maize', 0.0605501730294036),\n",
       " ('mamaliga', 0.0605501730294036),\n",
       " ('map', 0.0605501730294036),\n",
       " ('memory', 0.0605501730294036),\n",
       " ('midst', 0.0605501730294036),\n",
       " ('mina', 0.0605501730294036),\n",
       " ('mixed', 0.0605501730294036),\n",
       " ('moldavia', 0.0605501730294036),\n",
       " ('mountains', 0.0605501730294036),\n",
       " ('move', 0.0605501730294036),\n",
       " ('museum', 0.0605501730294036),\n",
       " ('nationalities', 0.0605501730294036),\n",
       " ('night', 0.0605501730294036),\n",
       " ('nobleman', 0.0605501730294036),\n",
       " ('north', 0.0605501730294036),\n",
       " ('notes', 0.0605501730294036),\n",
       " ('ordnance', 0.0605501730294036),\n",
       " ('place', 0.0605501730294036),\n",
       " ('plant', 0.0605501730294036),\n",
       " ('population', 0.0605501730294036),\n",
       " ('porridge', 0.0605501730294036),\n",
       " ('portions', 0.0605501730294036),\n",
       " ('post', 0.0605501730294036),\n",
       " ('queer', 0.0605501730294036),\n",
       " ('read', 0.0605501730294036),\n",
       " ('recipe', 0.0605501730294036),\n",
       " ('refresh', 0.0605501730294036),\n",
       " ('regarding', 0.0605501730294036),\n",
       " ('rushing', 0.0605501730294036),\n",
       " ('saxons', 0.0605501730294036),\n",
       " ('search', 0.0605501730294036),\n",
       " ('seems', 0.0605501730294036),\n",
       " ('settled', 0.0605501730294036),\n",
       " ('shall', 0.0605501730294036),\n",
       " ('sit', 0.0605501730294036),\n",
       " ('slept', 0.0605501730294036),\n",
       " ('sorts', 0.0605501730294036),\n",
       " ('soundly', 0.0605501730294036),\n",
       " ('south', 0.0605501730294036),\n",
       " ('started', 0.0605501730294036),\n",
       " ('station', 0.0605501730294036),\n",
       " ('stay', 0.0605501730294036),\n",
       " ('struck', 0.0605501730294036),\n",
       " ('stuffed', 0.0605501730294036),\n",
       " ('superstition', 0.0605501730294036),\n",
       " ('survey', 0.0605501730294036),\n",
       " ('szekelys', 0.0605501730294036),\n",
       " ('thirsty', 0.0605501730294036),\n",
       " ('train', 0.0605501730294036),\n",
       " ('trains', 0.0605501730294036),\n",
       " ('travels', 0.0605501730294036),\n",
       " ('unpunctual', 0.0605501730294036),\n",
       " ('visited', 0.0605501730294036),\n",
       " ('wakened', 0.0605501730294036),\n",
       " ('wallachs', 0.0605501730294036),\n",
       " ('water', 0.0605501730294036),\n",
       " ('west', 0.0605501730294036),\n",
       " ('whirlpool', 0.0605501730294036),\n",
       " ('wildest', 0.0605501730294036),\n",
       " ('work', 0.0605501730294036),\n",
       " ('world', 0.0605501730294036),\n",
       " ('found', 0.0653476566547517),\n",
       " ('must', 0.08592431547216046),\n",
       " ('well', 0.08592431547216046),\n",
       " ('among', 0.1211003460588072),\n",
       " ('breakfast', 0.1211003460588072),\n",
       " ('count', 0.1211003460588072),\n",
       " ('dracula', 0.1211003460588072),\n",
       " ('huns', 0.1211003460588072),\n",
       " ('magyars', 0.1211003460588072),\n",
       " ('mem', 0.1211003460588072),\n",
       " ('named', 0.1211003460588072),\n",
       " ('ought', 0.1211003460588072),\n",
       " ('paprika', 0.1211003460588072),\n",
       " ('sort', 0.1211003460588072),\n",
       " ('known', 0.1288864732082407),\n",
       " ('east', 0.1816505190882108),\n",
       " ('maps', 0.1816505190882108),\n",
       " ('transylvania', 0.1816505190882108),\n",
       " ('country', 0.302750865147018),\n",
       " ('may', 0.302750865147018)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list_document3, key = lambda t:t[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one above comes in increasing order. What if we need the opposite?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('country', 0.302750865147018),\n",
       " ('may', 0.302750865147018),\n",
       " ('east', 0.1816505190882108),\n",
       " ('maps', 0.1816505190882108),\n",
       " ('transylvania', 0.1816505190882108),\n",
       " ('known', 0.1288864732082407),\n",
       " ('among', 0.1211003460588072),\n",
       " ('breakfast', 0.1211003460588072),\n",
       " ('count', 0.1211003460588072),\n",
       " ('dracula', 0.1211003460588072),\n",
       " ('huns', 0.1211003460588072),\n",
       " ('magyars', 0.1211003460588072),\n",
       " ('mem', 0.1211003460588072),\n",
       " ('named', 0.1211003460588072),\n",
       " ('ought', 0.1211003460588072),\n",
       " ('paprika', 0.1211003460588072),\n",
       " ('sort', 0.1211003460588072),\n",
       " ('must', 0.08592431547216046),\n",
       " ('well', 0.08592431547216046),\n",
       " ('found', 0.0653476566547517),\n",
       " ('30', 0.0605501730294036),\n",
       " ('also', 0.0605501730294036),\n",
       " ('ask', 0.0605501730294036),\n",
       " ('attila', 0.0605501730294036),\n",
       " ('bistritz', 0.0605501730294036),\n",
       " ('books', 0.0605501730294036),\n",
       " ('borders', 0.0605501730294036),\n",
       " ('british', 0.0605501730294036),\n",
       " ('bukovina', 0.0605501730294036),\n",
       " ('call', 0.0605501730294036),\n",
       " ('carafe', 0.0605501730294036),\n",
       " ('carpathian', 0.0605501730294036),\n",
       " ('carpathians', 0.0605501730294036),\n",
       " ('carriage', 0.0605501730294036),\n",
       " ('castle', 0.0605501730294036),\n",
       " ('centre', 0.0605501730294036),\n",
       " ('century', 0.0605501730294036),\n",
       " ('china', 0.0605501730294036),\n",
       " ('claim', 0.0605501730294036),\n",
       " ('comfortable', 0.0605501730294036),\n",
       " ('compare', 0.0605501730294036),\n",
       " ('conquered', 0.0605501730294036),\n",
       " ('continuous', 0.0605501730294036),\n",
       " ('dacians', 0.0605501730294036),\n",
       " ('dealing', 0.0605501730294036),\n",
       " ('descendants', 0.0605501730294036),\n",
       " ('descended', 0.0605501730294036),\n",
       " ('dish', 0.0605501730294036),\n",
       " ('disposal', 0.0605501730294036),\n",
       " ('distinct', 0.0605501730294036),\n",
       " ('district', 0.0605501730294036),\n",
       " ('dog', 0.0605501730294036),\n",
       " ('done', 0.0605501730294036),\n",
       " ('door', 0.0605501730294036),\n",
       " ('drink', 0.0605501730294036),\n",
       " ('egg', 0.0605501730294036),\n",
       " ('eleventh', 0.0605501730294036),\n",
       " ('enough', 0.0605501730294036),\n",
       " ('enter', 0.0605501730294036),\n",
       " ('europe', 0.0605501730294036),\n",
       " ('every', 0.0605501730294036),\n",
       " ('exact', 0.0605501730294036),\n",
       " ('excellent', 0.0605501730294036),\n",
       " ('extreme', 0.0605501730294036),\n",
       " ('fail', 0.0605501730294036),\n",
       " ('fairly', 0.0605501730294036),\n",
       " ('flour', 0.0605501730294036),\n",
       " ('forcemeat', 0.0605501730294036),\n",
       " ('foreknowledge', 0.0605501730294036),\n",
       " ('gathered', 0.0605501730294036),\n",
       " ('giving', 0.0605501730294036),\n",
       " ('going', 0.0605501730294036),\n",
       " ('guess', 0.0605501730294036),\n",
       " ('horseshoe', 0.0605501730294036),\n",
       " ('hour', 0.0605501730294036),\n",
       " ('howling', 0.0605501730294036),\n",
       " ('hurry', 0.0605501730294036),\n",
       " ('imaginative', 0.0605501730294036),\n",
       " ('impletata', 0.0605501730294036),\n",
       " ('importance', 0.0605501730294036),\n",
       " ('knocking', 0.0605501730294036),\n",
       " ('latter', 0.0605501730294036),\n",
       " ('least', 0.0605501730294036),\n",
       " ('library', 0.0605501730294036),\n",
       " ('light', 0.0605501730294036),\n",
       " ('locality', 0.0605501730294036),\n",
       " ('maize', 0.0605501730294036),\n",
       " ('mamaliga', 0.0605501730294036),\n",
       " ('map', 0.0605501730294036),\n",
       " ('memory', 0.0605501730294036),\n",
       " ('midst', 0.0605501730294036),\n",
       " ('mina', 0.0605501730294036),\n",
       " ('mixed', 0.0605501730294036),\n",
       " ('moldavia', 0.0605501730294036),\n",
       " ('mountains', 0.0605501730294036),\n",
       " ('move', 0.0605501730294036),\n",
       " ('museum', 0.0605501730294036),\n",
       " ('nationalities', 0.0605501730294036),\n",
       " ('night', 0.0605501730294036),\n",
       " ('nobleman', 0.0605501730294036),\n",
       " ('north', 0.0605501730294036),\n",
       " ('notes', 0.0605501730294036),\n",
       " ('ordnance', 0.0605501730294036),\n",
       " ('place', 0.0605501730294036),\n",
       " ('plant', 0.0605501730294036),\n",
       " ('population', 0.0605501730294036),\n",
       " ('porridge', 0.0605501730294036),\n",
       " ('portions', 0.0605501730294036),\n",
       " ('post', 0.0605501730294036),\n",
       " ('queer', 0.0605501730294036),\n",
       " ('read', 0.0605501730294036),\n",
       " ('recipe', 0.0605501730294036),\n",
       " ('refresh', 0.0605501730294036),\n",
       " ('regarding', 0.0605501730294036),\n",
       " ('rushing', 0.0605501730294036),\n",
       " ('saxons', 0.0605501730294036),\n",
       " ('search', 0.0605501730294036),\n",
       " ('seems', 0.0605501730294036),\n",
       " ('settled', 0.0605501730294036),\n",
       " ('shall', 0.0605501730294036),\n",
       " ('sit', 0.0605501730294036),\n",
       " ('slept', 0.0605501730294036),\n",
       " ('sorts', 0.0605501730294036),\n",
       " ('soundly', 0.0605501730294036),\n",
       " ('south', 0.0605501730294036),\n",
       " ('started', 0.0605501730294036),\n",
       " ('station', 0.0605501730294036),\n",
       " ('stay', 0.0605501730294036),\n",
       " ('struck', 0.0605501730294036),\n",
       " ('stuffed', 0.0605501730294036),\n",
       " ('superstition', 0.0605501730294036),\n",
       " ('survey', 0.0605501730294036),\n",
       " ('szekelys', 0.0605501730294036),\n",
       " ('thirsty', 0.0605501730294036),\n",
       " ('train', 0.0605501730294036),\n",
       " ('trains', 0.0605501730294036),\n",
       " ('travels', 0.0605501730294036),\n",
       " ('unpunctual', 0.0605501730294036),\n",
       " ('visited', 0.0605501730294036),\n",
       " ('wakened', 0.0605501730294036),\n",
       " ('wallachs', 0.0605501730294036),\n",
       " ('water', 0.0605501730294036),\n",
       " ('west', 0.0605501730294036),\n",
       " ('whirlpool', 0.0605501730294036),\n",
       " ('wildest', 0.0605501730294036),\n",
       " ('work', 0.0605501730294036),\n",
       " ('world', 0.0605501730294036),\n",
       " ('able', 0.04296215773608023),\n",
       " ('bed', 0.04296215773608023),\n",
       " ('began', 0.04296215773608023),\n",
       " ('dreams', 0.04296215773608023),\n",
       " ('eight', 0.04296215773608023),\n",
       " ('find', 0.04296215773608023),\n",
       " ('four', 0.04296215773608023),\n",
       " ('get', 0.04296215773608023),\n",
       " ('go', 0.04296215773608023),\n",
       " ('hardly', 0.04296215773608023),\n",
       " ('interesting', 0.04296215773608023),\n",
       " ('little', 0.04296215773608023),\n",
       " ('london', 0.04296215773608023),\n",
       " ('morning', 0.04296215773608023),\n",
       " ('rather', 0.04296215773608023),\n",
       " ('said', 0.04296215773608023),\n",
       " ('sleep', 0.04296215773608023),\n",
       " ('sleeping', 0.04296215773608023),\n",
       " ('states', 0.04296215773608023),\n",
       " ('still', 0.04296215773608023),\n",
       " ('talk', 0.04296215773608023),\n",
       " ('though', 0.04296215773608023),\n",
       " ('three', 0.04296215773608023),\n",
       " ('time', 0.04296215773608023),\n",
       " ('towards', 0.04296215773608023),\n",
       " ('town', 0.04296215773608023),\n",
       " ('window', 0.04296215773608023),\n",
       " ('could', 0.03267382832737585),\n",
       " ('something', 0.03267382832737585),\n",
       " ('made', 0.025374142442756865),\n",
       " ('one', 0.025374142442756865),\n",
       " ('1507', 0.0),\n",
       " ('1508', 0.0),\n",
       " ('1511', 0.0),\n",
       " ('1512', 0.0),\n",
       " ('1st', 0.0),\n",
       " ('accept', 0.0),\n",
       " ('accomplished', 0.0),\n",
       " ('acquaintance', 0.0),\n",
       " ('actors', 0.0),\n",
       " ('acts', 0.0),\n",
       " ('affections', 0.0),\n",
       " ('age', 0.0),\n",
       " ('agencies', 0.0),\n",
       " ('air', 0.0),\n",
       " ('allowed', 0.0),\n",
       " ('alluded', 0.0),\n",
       " ('almost', 0.0),\n",
       " ('alone', 0.0),\n",
       " ('along', 0.0),\n",
       " ('already', 0.0),\n",
       " ('although', 0.0),\n",
       " ('always', 0.0),\n",
       " ('appearance', 0.0),\n",
       " ('approved', 0.0),\n",
       " ('aptness', 0.0),\n",
       " ('aragon', 0.0),\n",
       " ('arches', 0.0),\n",
       " ('arising', 0.0),\n",
       " ('arm', 0.0),\n",
       " ('armour', 0.0),\n",
       " ('aside', 0.0),\n",
       " ('assistance', 0.0),\n",
       " ('attained', 0.0),\n",
       " ('austere', 0.0),\n",
       " ('back', 0.0),\n",
       " ('backward', 0.0),\n",
       " ('battle', 0.0),\n",
       " ('beaconed', 0.0),\n",
       " ('bedding', 0.0),\n",
       " ('belly', 0.0),\n",
       " ('best', 0.0),\n",
       " ('better', 0.0),\n",
       " ('bit', 0.0),\n",
       " ('blood', 0.0),\n",
       " ('boa', 0.0),\n",
       " ('bond', 0.0),\n",
       " ('brasses', 0.0),\n",
       " ('broke', 0.0),\n",
       " ('brother', 0.0),\n",
       " ('brown', 0.0),\n",
       " ('business', 0.0),\n",
       " ('busy', 0.0),\n",
       " ('cain', 0.0),\n",
       " ('called', 0.0),\n",
       " ('calls', 0.0),\n",
       " ('cambrai', 0.0),\n",
       " ('came', 0.0),\n",
       " ('career', 0.0),\n",
       " ('carry', 0.0),\n",
       " ('catholicity', 0.0),\n",
       " ('caught', 0.0),\n",
       " ('chambers', 0.0),\n",
       " ('chanced', 0.0),\n",
       " ('change', 0.0),\n",
       " ('character', 0.0),\n",
       " ('charms', 0.0),\n",
       " ('chief', 0.0),\n",
       " ('circle', 0.0),\n",
       " ('cleanliness', 0.0),\n",
       " ('cloak', 0.0),\n",
       " ('cold', 0.0),\n",
       " ('collection', 0.0),\n",
       " ('common', 0.0),\n",
       " ('comparatively', 0.0),\n",
       " ('compared', 0.0),\n",
       " ('complicated', 0.0),\n",
       " ('concerned', 0.0),\n",
       " ('consequent', 0.0),\n",
       " ('contrast', 0.0),\n",
       " ('controlled', 0.0),\n",
       " ('coquetry', 0.0),\n",
       " ('counted', 0.0),\n",
       " ('countenance', 0.0),\n",
       " ('court', 0.0),\n",
       " ('cover', 0.0),\n",
       " ('covered', 0.0),\n",
       " ('crack', 0.0),\n",
       " ('crossed', 0.0),\n",
       " ('crushing', 0.0),\n",
       " ('cut', 0.0),\n",
       " ('day', 0.0),\n",
       " ('demeanour', 0.0),\n",
       " ('describes', 0.0),\n",
       " ('devil', 0.0),\n",
       " ('dictated', 0.0),\n",
       " ('died', 0.0),\n",
       " ('difficult', 0.0),\n",
       " ('dingy', 0.0),\n",
       " ('dinner', 0.0),\n",
       " ('discourse', 0.0),\n",
       " ('dismissal', 0.0),\n",
       " ('distant', 0.0),\n",
       " ('divided', 0.0),\n",
       " ('domed', 0.0),\n",
       " ('doors', 0.0),\n",
       " ('doubt', 0.0),\n",
       " ('downgoing', 0.0),\n",
       " ('drank', 0.0),\n",
       " ('drawn', 0.0),\n",
       " ('dream', 0.0),\n",
       " ('dreary', 0.0),\n",
       " ('drops', 0.0),\n",
       " ('drove', 0.0),\n",
       " ('dull', 0.0),\n",
       " ('dusty', 0.0),\n",
       " ('easy', 0.0),\n",
       " ('effect', 0.0),\n",
       " ('embarrassed', 0.0),\n",
       " ('eminently', 0.0),\n",
       " ('emperor', 0.0),\n",
       " ('empty', 0.0),\n",
       " ('emulously', 0.0),\n",
       " ('encountered', 0.0),\n",
       " ('end', 0.0),\n",
       " ('enfield', 0.0),\n",
       " ('enjoy', 0.0),\n",
       " ('enjoyed', 0.0),\n",
       " ('entire', 0.0),\n",
       " ('envoy', 0.0),\n",
       " ('envy', 0.0),\n",
       " ('estimate', 0.0),\n",
       " ('european', 0.0),\n",
       " ('even', 0.0),\n",
       " ('events', 0.0),\n",
       " ('excursions', 0.0),\n",
       " ('extremity', 0.0),\n",
       " ('eye', 0.0),\n",
       " ('eyes', 0.0),\n",
       " ('face', 0.0),\n",
       " ('failures', 0.0),\n",
       " ('faith', 0.0),\n",
       " ('fall', 0.0),\n",
       " ('familiar', 0.0),\n",
       " ('far', 0.0),\n",
       " ('feat', 0.0),\n",
       " ('feel', 0.0),\n",
       " ('felt', 0.0),\n",
       " ('ferdinand', 0.0),\n",
       " ('feud', 0.0),\n",
       " ('filled', 0.0),\n",
       " ('finally', 0.0),\n",
       " ('fire', 0.0),\n",
       " ('fitted', 0.0),\n",
       " ('florence', 0.0),\n",
       " ('florid', 0.0),\n",
       " ('floundering', 0.0),\n",
       " ('follow', 0.0),\n",
       " ('force', 0.0),\n",
       " ('forest', 0.0),\n",
       " ('forget', 0.0),\n",
       " ('formed', 0.0),\n",
       " ('fortune', 0.0),\n",
       " ('fortunes', 0.0),\n",
       " ('founded', 0.0),\n",
       " ('frame', 0.0),\n",
       " ('france', 0.0),\n",
       " ('french', 0.0),\n",
       " ('frequently', 0.0),\n",
       " ('freshly', 0.0),\n",
       " ('friend', 0.0),\n",
       " ('friendly', 0.0),\n",
       " ('friends', 0.0),\n",
       " ('friendship', 0.0),\n",
       " ('fronts', 0.0),\n",
       " ('fulfilment', 0.0),\n",
       " ('fur', 0.0),\n",
       " ('gaiety', 0.0),\n",
       " ('general', 0.0),\n",
       " ('germany', 0.0),\n",
       " ('gilded', 0.0),\n",
       " ('gin', 0.0),\n",
       " ('good', 0.0),\n",
       " ('grains', 0.0),\n",
       " ('great', 0.0),\n",
       " ('greatest', 0.0),\n",
       " ('gregor', 0.0),\n",
       " ('growth', 0.0),\n",
       " ('hail', 0.0),\n",
       " ('hands', 0.0),\n",
       " ('happened', 0.0),\n",
       " ('hard', 0.0),\n",
       " ('hat', 0.0),\n",
       " ('head', 0.0),\n",
       " ('heard', 0.0),\n",
       " ('heavy', 0.0),\n",
       " ('help', 0.0),\n",
       " ('helplessly', 0.0),\n",
       " ('hence', 0.0),\n",
       " ('heresy', 0.0),\n",
       " ('high', 0.0),\n",
       " ('hitting', 0.0),\n",
       " ('holy', 0.0),\n",
       " ('hoping', 0.0),\n",
       " ('horrible', 0.0),\n",
       " ('housed', 0.0),\n",
       " ('however', 0.0),\n",
       " ('human', 0.0),\n",
       " ('humanity', 0.0),\n",
       " ('hundred', 0.0),\n",
       " ('hung', 0.0),\n",
       " ('ignoring', 0.0),\n",
       " ('ii', 0.0),\n",
       " ('illustrated', 0.0),\n",
       " ('impinge', 0.0),\n",
       " ('implied', 0.0),\n",
       " ('impossible', 0.0),\n",
       " ('incline', 0.0),\n",
       " ('inclined', 0.0),\n",
       " ('indeed', 0.0),\n",
       " ('influence', 0.0),\n",
       " ('influenced', 0.0),\n",
       " ('inhabitants', 0.0),\n",
       " ('insisting', 0.0),\n",
       " ('instantly', 0.0),\n",
       " ('integrity', 0.0),\n",
       " ('invitation', 0.0),\n",
       " ('involved', 0.0),\n",
       " ('italian', 0.0),\n",
       " ('italy', 0.0),\n",
       " ('ivy', 0.0),\n",
       " ('jewel', 0.0),\n",
       " ('julius', 0.0),\n",
       " ('kinsman', 0.0),\n",
       " ('lady', 0.0),\n",
       " ('last', 0.0),\n",
       " ('lasted', 0.0),\n",
       " ('lawyer', 0.0),\n",
       " ('lay', 0.0),\n",
       " ('laying', 0.0),\n",
       " ('league', 0.0),\n",
       " ('lean', 0.0),\n",
       " ('led', 0.0),\n",
       " ('legs', 0.0),\n",
       " ('let', 0.0),\n",
       " ('life', 0.0),\n",
       " ('lifted', 0.0),\n",
       " ('lighted', 0.0),\n",
       " ('like', 0.0),\n",
       " ('lives', 0.0),\n",
       " ('long', 0.0),\n",
       " ('longer', 0.0),\n",
       " ('longest', 0.0),\n",
       " ('look', 0.0),\n",
       " ('looked', 0.0),\n",
       " ('lost', 0.0),\n",
       " ('loudly', 0.0),\n",
       " ('louis', 0.0),\n",
       " ('lovable', 0.0),\n",
       " ('lower', 0.0),\n",
       " ('machiavelli', 0.0),\n",
       " ('magazine', 0.0),\n",
       " ('man', 0.0),\n",
       " ('many', 0.0),\n",
       " ('mark', 0.0),\n",
       " ('marked', 0.0),\n",
       " ('maximilian', 0.0),\n",
       " ('medici', 0.0),\n",
       " ('meetings', 0.0),\n",
       " ('men', 0.0),\n",
       " ('mentioned', 0.0),\n",
       " ('mercy', 0.0),\n",
       " ('might', 0.0),\n",
       " ('mild', 0.0),\n",
       " ('misdeeds', 0.0),\n",
       " ('modest', 0.0),\n",
       " ('moment', 0.0),\n",
       " ('monarch', 0.0),\n",
       " ('mortify', 0.0),\n",
       " ('motives', 0.0),\n",
       " ('mr', 0.0),\n",
       " ('muff', 0.0),\n",
       " ('nature', 0.0),\n",
       " ('necessary', 0.0),\n",
       " ('neighbourhood', 0.0),\n",
       " ('never', 0.0),\n",
       " ('nice', 0.0),\n",
       " ('nonsense', 0.0),\n",
       " ('note', 0.0),\n",
       " ('nothing', 0.0),\n",
       " ('nut', 0.0),\n",
       " ('object', 0.0),\n",
       " ('obvious', 0.0),\n",
       " ('occasions', 0.0),\n",
       " ('office', 0.0),\n",
       " ('official', 0.0),\n",
       " ('often', 0.0),\n",
       " ('onto', 0.0),\n",
       " ('opportunity', 0.0),\n",
       " ('others', 0.0),\n",
       " ('pain', 0.0),\n",
       " ('painted', 0.0),\n",
       " ('pane', 0.0),\n",
       " ('part', 0.0),\n",
       " ('passage', 0.0),\n",
       " ('passenger', 0.0),\n",
       " ('peacefully', 0.0),\n",
       " ('personality', 0.0),\n",
       " ('picture', 0.0),\n",
       " ('pitifully', 0.0),\n",
       " ('play', 0.0),\n",
       " ('pleased', 0.0),\n",
       " ('pleasure', 0.0),\n",
       " ('policy', 0.0),\n",
       " ('polished', 0.0),\n",
       " ('pope', 0.0),\n",
       " ('position', 0.0),\n",
       " ('powers', 0.0),\n",
       " ('present', 0.0),\n",
       " ('pressure', 0.0),\n",
       " ('proper', 0.0),\n",
       " ('public', 0.0),\n",
       " ('put', 0.0),\n",
       " ('quaintly', 0.0),\n",
       " ('quarter', 0.0),\n",
       " ('quiet', 0.0),\n",
       " ('quite', 0.0),\n",
       " ('rain', 0.0),\n",
       " ('raising', 0.0),\n",
       " ('rambles', 0.0),\n",
       " ('ready', 0.0),\n",
       " ('reality', 0.0),\n",
       " ('recently', 0.0),\n",
       " ('regaining', 0.0),\n",
       " ('relief', 0.0),\n",
       " ('religion', 0.0),\n",
       " ('remaining', 0.0),\n",
       " ('reported', 0.0),\n",
       " ('reprove', 0.0),\n",
       " ('republic', 0.0),\n",
       " ('reputable', 0.0),\n",
       " ('resisted', 0.0),\n",
       " ('rest', 0.0),\n",
       " ('restored', 0.0),\n",
       " ('result', 0.0),\n",
       " ('results', 0.0),\n",
       " ('return', 0.0),\n",
       " ('reveals', 0.0),\n",
       " ('richard', 0.0),\n",
       " ('right', 0.0),\n",
       " ('rolled', 0.0),\n",
       " ('room', 0.0),\n",
       " ('rows', 0.0),\n",
       " ('rugged', 0.0),\n",
       " ('ruined', 0.0),\n",
       " ('sad', 0.0),\n",
       " ('salesman', 0.0),\n",
       " ('saleswomen', 0.0),\n",
       " ('samples', 0.0),\n",
       " ('samsa', 0.0),\n",
       " ('sat', 0.0),\n",
       " ('say', 0.0),\n",
       " ('scanty', 0.0),\n",
       " ('schemes', 0.0),\n",
       " ('secret', 0.0),\n",
       " ('secretive', 0.0),\n",
       " ('sections', 0.0),\n",
       " ('see', 0.0),\n",
       " ('seemed', 0.0),\n",
       " ('seen', 0.0),\n",
       " ('sentiment', 0.0),\n",
       " ('september', 0.0),\n",
       " ('set', 0.0),\n",
       " ('several', 0.0),\n",
       " ('shade', 0.0),\n",
       " ('shone', 0.0),\n",
       " ('shop', 0.0),\n",
       " ('showed', 0.0),\n",
       " ('shut', 0.0),\n",
       " ('shutters', 0.0),\n",
       " ('signal', 0.0),\n",
       " ('silent', 0.0),\n",
       " ('similar', 0.0),\n",
       " ('singularly', 0.0),\n",
       " ('size', 0.0),\n",
       " ('slide', 0.0),\n",
       " ('slightly', 0.0),\n",
       " ('small', 0.0),\n",
       " ('smile', 0.0),\n",
       " ('smiling', 0.0),\n",
       " ('somehow', 0.0),\n",
       " ('sometimes', 0.0),\n",
       " ('spain', 0.0),\n",
       " ('spirits', 0.0),\n",
       " ('spoke', 0.0),\n",
       " ('spread', 0.0),\n",
       " ('state', 0.0),\n",
       " ('stiff', 0.0),\n",
       " ('stood', 0.0),\n",
       " ('stopped', 0.0),\n",
       " ('store', 0.0),\n",
       " ('street', 0.0),\n",
       " ('subject', 0.0),\n",
       " ('submit', 0.0),\n",
       " ('sunday', 0.0),\n",
       " ('surplus', 0.0),\n",
       " ('swiss', 0.0),\n",
       " ('symbols', 0.0),\n",
       " ('table', 0.0),\n",
       " ('taste', 0.0),\n",
       " ('terms', 0.0),\n",
       " ('textile', 0.0),\n",
       " ('theatre', 0.0),\n",
       " ('thin', 0.0),\n",
       " ('things', 0.0),\n",
       " ('thoroughfare', 0.0),\n",
       " ('thought', 0.0),\n",
       " ('threw', 0.0),\n",
       " ('thriving', 0.0),\n",
       " ('thus', 0.0),\n",
       " ('times', 0.0),\n",
       " ('tolerance', 0.0),\n",
       " ('trade', 0.0),\n",
       " ('transformed', 0.0),\n",
       " ('travelling', 0.0),\n",
       " ('tried', 0.0),\n",
       " ('troubled', 0.0),\n",
       " ('turned', 0.0),\n",
       " ('twenty', 0.0),\n",
       " ('two', 0.0),\n",
       " ('unable', 0.0),\n",
       " ('undemonstrative', 0.0),\n",
       " ('uninterrupted', 0.0),\n",
       " ('united', 0.0),\n",
       " ('upright', 0.0),\n",
       " ('used', 0.0),\n",
       " ('utterson', 0.0),\n",
       " ('vaila', 0.0),\n",
       " ('varying', 0.0),\n",
       " ('veiled', 0.0),\n",
       " ('venetian', 0.0),\n",
       " ('venice', 0.0),\n",
       " ('vermin', 0.0),\n",
       " ('viewer', 0.0),\n",
       " ('vintages', 0.0),\n",
       " ('walks', 0.0),\n",
       " ('walls', 0.0),\n",
       " ('waved', 0.0),\n",
       " ('way', 0.0),\n",
       " ('weather', 0.0),\n",
       " ('week', 0.0),\n",
       " ('weekdays', 0.0),\n",
       " ('whole', 0.0),\n",
       " ('wine', 0.0),\n",
       " ('wishes', 0.0),\n",
       " ('without', 0.0),\n",
       " ('woke', 0.0),\n",
       " ('wondering', 0.0),\n",
       " ('would', 0.0),\n",
       " ('xii', 0.0),\n",
       " ('years', 0.0)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sorted(list_document3, key = lambda t:t[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
