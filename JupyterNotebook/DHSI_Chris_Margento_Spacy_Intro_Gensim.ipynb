{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Margento: SpaCy & Intro to Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "SPACY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER, and word vectors\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "spaCy is able to compare two objects, and make a prediction of how similar they are. Predicting similarity is useful for building recommendation systems or flagging duplicates. For example, you can suggest a user content that's similar to what they're currently looking at, or label a support ticket as a duplicate if it's very similar to an already existing one.\n",
    "\n",
    "Each Doc, Span, and Token comes with a .similarity()  method that lets you compare it with another object, and determine the similarity. Of course similarity is always subjective – whether \"dog\" and \"cat\" are similar really depends on how you're looking at it. spaCy's similarity model usually assumes a pretty general-purpose definition of similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(u'machine computer human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(u'cat dog banana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "LET US COMPARE THIS WITH ANOTHER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp2 = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp2(u'machine computer human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the previous model outputted:\n",
    "\n",
    "1.0\n",
    "0.6076606\n",
    "0.20924059\n",
    "0.6076606\n",
    "1.0\n",
    "0.40710005\n",
    "0.20924059\n",
    "0.40710005\n",
    "1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting differences, don't you think? What is happening here? \"lg\" actually stands for large... (so it is a larger version of the same model actually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(u'machine computer husband wife')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokens2 = nlp2(u'machine computer husband wife')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token1 in tokens2:\n",
    "    for token2 in tokens2:\n",
    "        print(token1.similarity(token2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp2(u'cat dog banana')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LET US REPEAT THE EVALUATION (the comparison between the three tokens above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the output of the previous model (in fact the small version of the same model) was:\n",
    "1.0\n",
    "0.4759995\n",
    "0.2811343\n",
    "0.4759995\n",
    "1.0\n",
    "0.51964444\n",
    "0.2811343\n",
    "0.51964444\n",
    "1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "SIMILARITIES IN CONTEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from spaCy's built-in word vectors, which were trained on a lot of text with a wide vocabulary, the parsing, tagging and NER models also rely on vector representations of the meanings of words in context. As the processing pipeline is applied spaCy encodes a document's internal meaning representations as an array of floats, also called a tensor. This allows spaCy to make a reasonable guess at a word's meaning, based on its surrounding words. Even if a word hasn't been seen before, spaCy will know something about it. Because spaCy uses a 4-layer convolutional network, the tensors are sensitive to up to four words on either side of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, here are three sentences containing the out-of-vocabulary word \"labrador\" in different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(u\"The labrador barked.\")\n",
    "doc2 = nlp(u\"The labrador swam.\")\n",
    "doc3 = nlp(u\"the labrador people live in canada.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Even though the model has never seen the word \"labrador\", it can make a fairly accurate prediction of its similarity to \"dog\" in different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in [doc1, doc2, doc3]:\n",
    "    labrador = doc[1]\n",
    "    dog = nlp(u\"dog\")\n",
    "    print(labrador.similarity(dog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp2 = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp2(u\"The labrador barked.\")\n",
    "doc2 = nlp2(u\"The labrador swam.\")\n",
    "doc3 = nlp2(u\"the labrador people live in canada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in [doc1, doc2, doc3]:\n",
    "    labrador = doc[1]\n",
    "    dog = nlp2(u\"dog\")\n",
    "    print(labrador.similarity(dog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider an even more extraneous word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(u\"The vaca is a cow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u\"The vaca is a rocket flying in the sky.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(u\"The vaca teaches digital humanities.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in [doc1, doc2, doc3]:\n",
    "    vaca = doc[1]\n",
    "    cow = nlp(u\"cow\")\n",
    "    print(vaca.similarity(cow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp2(u\"The vaca is a Romanian cow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp2(u\"The vaca is a rocket flying in the sky.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp2(u\"The vaca teaches digital humanities.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in [doc1, doc2, doc3]:\n",
    "    vaca = doc[1]\n",
    "    cow = nlp2(u\"cow\")\n",
    "    print(vaca.similarity(cow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE \n",
    "Please give an example of your own and let us see what happens ;) :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example and code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use SpaCy to compare documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(u\"Paris is the largest city in France.\")\n",
    "doc2 = nlp(u\"Bucharest is known as the Paris of the East.\")\n",
    "doc3 = nlp(u\"It is raining cats and dogs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in [doc1, doc2, doc3]:\n",
    "    for other_doc in [doc1, doc2, doc3]:\n",
    "        print(doc.similarity(other_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doca = nlp2(u\"Paris is the largest city in France.\")\n",
    "docb = nlp2(u\"Bucharest is known as the Paris of the East.\")\n",
    "docc = nlp2(u\"It is raining cats and dogs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in [doca, docb, docc]:\n",
    "    for other_doc in [doca, docb, docc]:\n",
    "        print(doc.similarity(other_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run this on the documents we compared earlier with Sklearn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc0 = nlp(u\"I am interested in NLP and would like to learn more about vectors and vectorizers for language processing.\")\n",
    "doc1 = nlp(u\"For those who like to experiment with vectors in dealing with various data, it might be useful to work with NLP vectorizers.\")\n",
    "doc2 = nlp(u\"A significant computational experiment involving language will have to involve vectors, so you will want to look into vectorizers.\")\n",
    "doc3 = nlp(u\"Even if you are not interested in NLP, you can use vectors to organize your data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in [doc0, doc1, doc2, doc3]:\n",
    "    for other_doc in [doc0, doc1, doc2, doc3]:\n",
    "        print(doc.similarity(other_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doca = nlp2(u\"I am interested in NLP and would like to learn more about vectors and vectorizers for language processing.\")\n",
    "docb = nlp2(u\"For those who like to experiment with vectors in dealing with various data, it might be useful to work with NLP vectorizers.\")\n",
    "docc = nlp2(u\"A significant computational experiment involving language will have to involve vectors, so you will want to look into vectorizers.\")\n",
    "docd = nlp2(u\"Even if you are not interested in NLP, you can use vectors to organize your data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in [doca, docb, docc, docd]:\n",
    "    for other_doc in [doca, docb, docc, docd]:\n",
    "        print(doc.similarity(other_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Do we have the means in scikitlearn to compare documents that directly? Let us go back and have a look. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The similarity matrix in the NLP--Intro notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[(1.        ,) (0.23144825,) (0.17254319,) (0.19211725,)]\n",
    " [(0.23144825,) (1.        ,) (0.11253336,) (0.298293  ,)]\n",
    " [(0.17254319,) (0.11253336,) (1.        ,) (0.02081005,)]\n",
    " [(0.19211725,) (0.298293  ,) (0.02081005,) (1.        ,)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"I am interested in NLP and would like to learn more about vectors and vectorizers for language processing.\"\n",
    "s2 = \"For those who like to experiment with vectors in dealing with various data, it might be useful to work with NLP vectorizers.\"\n",
    "s3 = \"A significant computational experiment involving language will have to involve vectors, so you will want to look into vectorizers.\"\n",
    "s4 = \"Even if you are not interested in NLP, you can use vectors to organize your data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"I am interested in NLP and would like to learn more about vectors and vectorizers for language processing.\", \"For those who like to experiment with vectors in dealing with various data, it might be useful to work with NLP vectorizers.\", \"A significant computational experiment involving language will have to involve vectors, so you will want to look into vectorizers.\", \"Even if you are not interested in NLP, you can use vectors to organize your data.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        #elif token.like_url:\n",
    "            #lda_tokens.append('URL')\n",
    "        #elif token.orth_.startswith('@'):\n",
    "            #lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "\n",
    "for f in texts:\n",
    "            tokens = prepare_text_for_lda(f)\n",
    "            text_data.append(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "import pickle # Pickle in Python is primarily used in serializing and deserializing a Python object structure. In other words, it's the process of converting a Python object into a byte stream to store it in a file/database, maintain program state across sessions, or transport data over the network.\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb')) # Returns the reconstructed Python object from the pickle source. The pickle source could be a file or a memory buffer. The dump() method of the pickle module in Python, converts a Python object hierarchy into a byte stream. This process is also called as serialization.\n",
    "# The converted byte stream can be written to a buffer or to a disk file. The byte stream of a pickled Python object can converted back to a Python object using the pickle.load() method.\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "index = similarities.MatrixSimilarity(ldamodel[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.save(\"simIndex.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s2 = text_data[1]\n",
    "vec_bow = dictionary.doc2bow(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_lda = ldamodel[vec_bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = index[vec_lda]\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('stop_words_poetry.txt')\n",
    "\n",
    "stopwords.append('...')\n",
    "stopwords.append(\"'d\")\n",
    "stopwords.append('...')\n",
    "stopwords.append(\"&\")\n",
    "stopwords.append(\"upon\")\n",
    "stopwords.append(\"also\")\n",
    "stopwords.append(\"hath\")\n",
    "stopwords.append(\"must\")\n",
    "stopwords.append(\"therefore\")\n",
    "stopwords.append(\"doth\")\n",
    "stopwords.append(\"could\")\n",
    "stopwords.append(\"would\")\n",
    "#stopwords.append(\"another\")\n",
    "stopwords.append(\"much\")\n",
    "#stopwords.append(\"give\")\n",
    "stopwords.append(\"like\")\n",
    "stopwords.append(\"since\")\n",
    "#stopwords.append(\"many\")\n",
    "#stopwords.append(\"without\")\n",
    "#stopwords.append(\"first\")\n",
    "stopwords.append(\"though\")\n",
    "#stopwords.append(\"well\")\n",
    "#stopwords.append(\"often\")\n",
    "#stopwords.append(\"great\")\n",
    "stopwords.append(\"either\")\n",
    "#stopwords.append(\"even\")\n",
    "stopwords.append(\"shall\")\n",
    "#stopwords.append(\"they\")\n",
    "stopwords.append(\"what\")\n",
    "stopwords.append(\"their\")\n",
    "#stopwords.append(\"more\")\n",
    "#stopwords.append(\"there\")\n",
    "#stopwords.append(\"your\")\n",
    "#stopwords.append(\"them\")\n",
    "stopwords.append(\"’\")\n",
    "stopwords.append(\"“\")\n",
    "stopwords.append(\"2\")\n",
    "stopwords.append(\"3\")\n",
    "stopwords.append(\"”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.extend(['a', 'like', 'you', 'they', 'he', 'be', 'it', 'your', 'her', 'of', 'more', 'there', 'no', 'not', '’', 'what', 'my', 'his', 'she', 'to', 'our', 'me', 'we', 'in', 'can', 'us', 'an', 'if', 'do', 'this', '”', 'because', 'who', 'hand', 'but', 'him'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pre_clean(list_of_text):\n",
    "        '''\n",
    "        preliminary cleaning of the text\n",
    "        - remove new line character i.e. \\n or \\r\n",
    "        - remove tabs i.e. \\t\n",
    "        - remove extra spaces\n",
    "        '''\n",
    "        cleaned_list = []\n",
    "        for text in list_of_text:\n",
    "            # print(\"original:\", text)\n",
    "            text = text.replace('\\\\n', ' ')\n",
    "            text = text.replace('\\\\r', ' ')\n",
    "            text = text.replace('\\\\t', ' ')\n",
    "            pattern = re.compile(r'\\s+')\n",
    "            text = re.sub(pattern, ' ', text)\n",
    "            text = text.strip()\n",
    "            # check for empty strings\n",
    "            if text != '' and text is not None:\n",
    "                cleaned_list.append(text)\n",
    "\n",
    "        return cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = _pre_clean(tokens)\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    #tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HOME = os.getcwd()\n",
    "\n",
    "TEXTS_DIR = HOME + \"/US_Poets_Anthology2/\"\n",
    "\n",
    "#TEXTS_DIR = HOME\n",
    "\n",
    "filelabels = {}\n",
    "\n",
    "texts_data = []\n",
    "\n",
    "files = [f for f in os.listdir(TEXTS_DIR) if os.path.isfile(os.path.join(TEXTS_DIR, f))]\n",
    "\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "tokens_total = []\n",
    "\n",
    "count = -1\n",
    " \n",
    "os.chdir(TEXTS_DIR)\n",
    "    \n",
    "for f in files:\n",
    "    #os.chdir(TEXTS_DIR)\n",
    "    with open(f, \"r\", encoding='utf-8', errors = 'ignore') as openf:\n",
    "        tokens = []\n",
    "        count = count + 1\n",
    "        filelabels[count] = os.path.basename(openf.name)\n",
    "        for line in openf:\n",
    "            sent_text = nltk.sent_tokenize(line)\n",
    "            for sentence in sent_text:\n",
    "                tokens1 = tokenize(sentence)\n",
    "                tokens1 = [item.translate(remove_punct_map)\n",
    "                      for item in tokens1]\n",
    "                #filter_object = filter(lambda x: x != \"\", tokens1)\n",
    "                tokens1 = [x for x in tokens1 if x!= \"\"]\n",
    "                tokens1 = [x.lower() for x in tokens1]\n",
    "                for token in tokens1:\n",
    "                    tokens.append(token)\n",
    "                    tokens_total.append(token)\n",
    "                #if random.random() > .99:\n",
    "                #print(tokens)\n",
    "    #print(tokens_total)\n",
    "    texts_data.append(tokens)\n",
    "\n",
    "print(filelabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts_data]\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')\n",
    "\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, id2word=dictionary)\n",
    "\n",
    "from gensim import similarities\n",
    "index = similarities.MatrixSimilarity(ldamodel[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.save(\"simIndex.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = texts_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vec_bow = dictionary.doc2bow(p0)\n",
    "\n",
    "vec_lda = ldamodel[vec_bow]\n",
    "\n",
    "sims = index[vec_lda]\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_data[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p33 = texts_data[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_bow = dictionary.doc2bow(p33)\n",
    "\n",
    "vec_lda = ldamodel[vec_bow]\n",
    "\n",
    "sims = index[vec_lda]\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stranger = text_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vec_bow = dictionary.doc2bow(p_stranger)\n",
    "\n",
    "vec_lda = ldamodel[vec_bow]\n",
    "\n",
    "sims = index[vec_lda]\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "Mikolov, Tomas, et al. 2013. \"Distributed Representations of Words and Phrases and their Compositionality\" \n",
    "Le, Quoc and Mikolov, Tomas. 2014. \"Distributed Representations of Sentences and Documents\n",
    "SpaCy. https://spacy.io/\n",
    "TensorFlow, \"Vector Representations of Words\"\n",
    "Brownlee, Jason. 2017. Machine Learning Mastery"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
