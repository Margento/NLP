{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import os\n",
    "#The OS module in Python provides a way of using operating system dependent functionality. \n",
    "#The functions that the OS module provides allows you to interface with the underlying operating system \n",
    "#that Python is running on – be that Windows, Mac or Linux.\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# import Gensim\n",
    "#import gensim\n",
    "#import gensim.corpora as corpora\n",
    "#from gensim import models, corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "#from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "#import spacy\n",
    "\n",
    "# Plotting tools\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nltk \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('stop_words_poetry.txt')\n",
    "\n",
    "stopwords.append('...')\n",
    "stopwords.append(\"'d\")\n",
    "stopwords.append('...')\n",
    "stopwords.append(\"&\")\n",
    "stopwords.append(\"upon\")\n",
    "stopwords.append(\"also\")\n",
    "stopwords.append(\"hath\")\n",
    "stopwords.append(\"must\")\n",
    "stopwords.append(\"therefore\")\n",
    "stopwords.append(\"doth\")\n",
    "stopwords.append(\"could\")\n",
    "stopwords.append(\"would\")\n",
    "stopwords.append(\"another\")\n",
    "stopwords.append(\"much\")\n",
    "stopwords.append(\"give\")\n",
    "stopwords.append(\"like\")\n",
    "stopwords.append(\"since\")\n",
    "stopwords.append(\"many\")\n",
    "stopwords.append(\"without\")\n",
    "stopwords.append(\"first\")\n",
    "stopwords.append(\"though\")\n",
    "stopwords.append(\"well\")\n",
    "stopwords.append(\"often\")\n",
    "#stopwords.append(\"great\")\n",
    "stopwords.append(\"either\")\n",
    "stopwords.append(\"even\")\n",
    "stopwords.append(\"shall\")\n",
    "stopwords.append(\"they\")\n",
    "stopwords.append(\"what\")\n",
    "stopwords.append(\"their\")\n",
    "stopwords.append(\"more\")\n",
    "stopwords.append(\"there\")\n",
    "stopwords.append(\"your\")\n",
    "stopwords.append(\"them\")\n",
    "stopwords.append(\"“\")\n",
    "stopwords.append(\"”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = _pre_clean(tokens)\n",
    "    tokens = [token for token in tokens if len(token) > 0]\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    #tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pre_clean(list_of_text):\n",
    "        '''\n",
    "        preliminary cleaning of the text\n",
    "        - remove new line character i.e. \\n or \\r\n",
    "        - remove tabs i.e. \\t\n",
    "        - remove extra spaces\n",
    "        '''\n",
    "        cleaned_list = []\n",
    "        for text in list_of_text:\n",
    "            # print(\"original:\", text)\n",
    "            text = text.replace('\\\\n', ' ')\n",
    "            text = text.replace('\\\\r', ' ')\n",
    "            text = text.replace('\\\\t', ' ')\n",
    "            pattern = re.compile(r'\\s+')\n",
    "            text = re.sub(pattern, ' ', text)\n",
    "            text = text.strip()\n",
    "            text = text.lower()\n",
    "            # check for empty strings\n",
    "            if text != '' and text is not None:\n",
    "                cleaned_list.append(text)\n",
    "\n",
    "        return cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HOME = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS_DIR = HOME + \"/Chaire_Altissia_LFIAL2010_1/\"\n",
    "\n",
    "# TEXTS_DIR = HOME\n",
    "\n",
    "filelabels = {}\n",
    "\n",
    "texts_data = []\n",
    "\n",
    "files = [f for f in os.listdir(TEXTS_DIR) if os.path.isfile(os.path.join(TEXTS_DIR, f))]\n",
    "\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "tokens_total = []\n",
    "\n",
    "count = -1\n",
    " \n",
    "os.chdir(TEXTS_DIR)\n",
    "    \n",
    "for f in files:\n",
    "    #os.chdir(TEXTS_DIR)\n",
    "    with open(f, \"r\", encoding='utf-8', errors = 'ignore') as openf:\n",
    "        tokens = []\n",
    "        count = count + 1\n",
    "        filelabels[count] = os.path.basename(openf.name)\n",
    "        for line in openf:\n",
    "            sent_text = nltk.sent_tokenize(line)\n",
    "            for sentence in sent_text:\n",
    "                tokens1 = tokenize(sentence)\n",
    "                tokens1 = [item.translate(remove_punct_map)\n",
    "                      for item in tokens1]\n",
    "                #filter_object = filter(lambda x: x != \"\", tokens1)\n",
    "                tokens1 = [x for x in tokens1 if x!= \"\"]\n",
    "                for token in tokens1:\n",
    "                    tokens.append(token)\n",
    "                    tokens_total.append(token)\n",
    "                #if random.random() > .99:\n",
    "                #print(tokens)\n",
    "    #print(tokens_total)\n",
    "    texts_data.append(tokens)\n",
    "\n",
    "print(filelabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(tokens_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_total = [x for x in tokens_total if x not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(tokens_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Count_total = Counter(tokens_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(Count_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyperclip as clip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.copy(f\"{Count_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command+V into a page/word/txt file [or clip.paste() to print it here, but in this case it is too large a list to print]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(13):\n",
    "    print(len(texts_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(13):\n",
    "    texts_data[i] = [x for x in texts_data[i] if x not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(13):\n",
    "    print(len(texts_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Count_total_0 = Counter(texts_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(Count_total_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
